{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyForestScan Documentation","text":"<p>Calculate Forest Structural Metrics from lidar point clouds in Python</p> <p></p>"},{"location":"#overview","title":"Overview","text":"<p>PyForestScan is a Python library designed for analyzing and visualizing forest structure using airborne 3D point cloud data. The library helps derive important forest metrics such as Canopy Height, Plant Area Index (PAI), Canopy Cover, Plant Area Density (PAD), and Foliage Height Diversity (FHD).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Forest Metrics: Calculate and visualize key metrics like Canopy Height, PAI, PAD, and FHD.</li> <li>Large Point Cloud Support: Utilizes efficient data formats such as EPT for large point cloud processing.</li> <li>Visualization: Create 2D and 3D visualizations of forest structure and structural metrics</li> <li>Extensibility: Easily add custom filters and visualization techniques to suit your needs.</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>The examples below are jupyter notebooks and can help you get started!</p> <ul> <li>Getting Started: DTM and CHM</li> <li>Calculating Forest Metrics</li> <li>Working with Large Point Clouds</li> </ul> <p>To install jupyter, you can use conda or pip, with either:</p> <p><pre><code>conda install jupyter\n</code></pre> or </p> <pre><code>pip install jupyter\n</code></pre>"},{"location":"#attribution","title":"Attribution","text":"<p>This library makes heavy use of PDAL (Butler et al. 2024; Butler et al. 2021) for its IO operations. PDAL and the PDAL Python Bindings provide excellent functional support for conducting standard operations on point clouds. Our work to calculate forest structural metrics would have been a lot harder without PDAL. </p> <p>Butler, H., Bell, A., Gerlek, M. P., chambbj, Gadomski, P., Manning, C., \u0141oskot, M., Couwenberg, B., Barker, N., Ramsey, P., Dark, J., Mann, K., Chaulet, N., Rouault, E., Villemin, G., Foster, C., Moore, O., Rosen, M., Lewis, S., ... Brookes, D. (2024). PDAL/PDAL: 2.8.1 (Version 2.8.1). Zenodo. https://doi.org/10.5281/zenodo.13993879</p> <p>Butler, H., Chambers, B., Hartzell, P., &amp; Glennie, C. (2021). PDAL: An open source library for the processing and analysis of point clouds. Computers &amp; Geosciences, 148, 104680. https://doi.org/https://doi.org/10.1016/j.cageo.2020.104680</p>"},{"location":"benchmarks/","title":"Benchmarking","text":"<p>PyForestScan is designed for high performance and memory efficiency, ensuring it can handle large-scale point cloud datasets effectively. While no other Python libraries specifically calculate these forest structure metrics, there are alternatives in R, such as the<code>leafR</code> library (Almeida et al. 2021), that offer similar functionality.</p> <p>We provide a direct performance comparison between PyForestScan and<code>leafR</code> to demonstrate its efficiency. In both cases, we calculate Plant Area Index (PAI) (this is labelled as Leaf Area Index in the <code>leafR</code> library) on a LAS tile, repeating the process 100 times and plotting the results. The measured benchmarking is only done on the functions to calculate PAI/LAI. It does not include time taken to load the point cloud, etc.</p> <p>The benchmarks were conducted on a Mac with an Apple M3 Max processor (16 cores) and 128GB RAM.</p> <p></p>"},{"location":"benchmarks/#code-used","title":"Code Used","text":"<p>To calculate LAI in <code>leafR</code>, and compare these with the python code, we:</p> <pre><code># Install and load required packages\nif (!require(\"lidR\")) install.packages(\"lidR\")\nif (!require(\"raster\")) install.packages(\"raster\")\nif (!require(\"leafR\")) install.packages(\"leafR\")\n\nlibrary(lidR)\nlibrary(raster)\nlibrary(leafR)\n\nfile_path &lt;- \"example_data/20191126_5QKB020840_normalized.laz\"\nlas &lt;- readLAS(file_path)\nif (is.empty(las)) {\n  stop(\"The LAS file is empty or could not be read.\")\n}\n\nif (is.null(las@data$Z)) {\n  stop(\"The LAS file does not contain Z coordinates.\")\n}\n\nif (!\"Zref\" %in% names(las@data)) {\n  las &lt;- normalize_height(las, tin())\n}\n\ntemp_las_file &lt;- tempfile(fileext = \".las\")\nwriteLAS(las, temp_las_file)\n\ncompute_lai &lt;- function(las_file_path) {\n  lad_voxels &lt;- lad.voxels(las_file_path, grain.size = 25)\n  lai_raster &lt;- lai.raster(lad_voxels)\n  return(lai_raster)\n}\n\ntiming_results &lt;- data.frame(software = character(),\n                             time = numeric(),\n                             stringsAsFactors = FALSE)\n\nfor (i in 1:100) {\n  start_time &lt;- Sys.time()\n  lai_result &lt;- compute_lai(temp_las_file)\n  end_time &lt;- Sys.time()\n  iteration_time &lt;- as.numeric(difftime(end_time, start_time, units = \"secs\"))\n\n  timing_results &lt;- rbind(timing_results, data.frame(software = \"R::leafR\", time = iteration_time))\n  cat(sprintf(\"Iteration %d completed in %.2f seconds.\\n\", i, iteration_time))\n}\n\nwrite.csv(timing_results, file = \"timing_results.csv\", row.names = FALSE)\n\ncat(sprintf(\"Total time for 100 iterations: %.2f seconds.\\n\", sum(timing_results$time)))\n\nunlink(temp_las_file)\n</code></pre>"},{"location":"benchmarks/#reference","title":"Reference","text":"<p>Almeida, Danilo Roberti Alves de, Scott Christopher Stark, Carlos Alberto Silva, Caio Hamamura, and Ruben Valbuena. 2021. \"leafR: Calculates the Leaf Area Index (LAD) and Other Related Functions.\"Manual. https://CRAN.R-project.org/package=leafR.</p>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting ipercival[at]gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"contributing/","title":"Contributing to PyForestScan","text":"<p>Thank you for contributing to PyForestScan! Your involvement helps make this project a great tool for point cloud data processing and visualization of forest structure.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, please follow our Code of Conduct.</p>"},{"location":"contributing/#how-can-i-contribute","title":"How Can I Contribute?","text":""},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<ul> <li> <p>Check Existing Issues \u2014 Before opening a new bug report, see if the issue has already been reported. If it has, add any additional details in a comment.</p> </li> <li> <p>Submit a Report \u2014 If the issue hasn't been reported, open a new issue and fill out the provided template.</p> </li> </ul>"},{"location":"contributing/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>Have an idea to improve PyForestScan? Please open an issue to discuss your suggestion.</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>To contribute via pull requests:</p> <ol> <li>Fork the Repository \u2014 Fork the PyForestScan repository and clone it locally.</li> <li>Create a Branch \u2014 Make changes in a new branch. Use descriptive names like <code>feat/</code>, <code>fix/</code>, or <code>docs/</code> followed by the feature or fix name.</li> <li>Commit Your Changes \u2014 Write a clear commit message describing your changes.</li> <li>Push to Your Fork \u2014 Push the branch to your fork on GitHub.</li> <li>Create a Pull Request \u2014 Open a pull request (PR) in the PyForestScan repository. Link any relevant issues.</li> <li>Code Review \u2014 A maintainer will review your changes. You may need to make updates based on feedback.</li> <li>Merge \u2014 Once approved, your PR will be merged into the main codebase.</li> </ol>"},{"location":"contributing/#style-guidelines","title":"Style Guidelines","text":""},{"location":"contributing/#python","title":"Python","text":"<ul> <li>Follow the PEP 8 style guide.</li> <li>Use type hints in functions.</li> <li>Add documentation to public APIs.</li> </ul>"},{"location":"contributing/#git-commit-messages","title":"Git Commit Messages","text":"<ul> <li>Use present tense (\"Add feature\" not \"Added feature\").</li> <li>Limit the first line to 72 characters or fewer.</li> <li>Reference related issues and PRs when relevant.</li> </ul>"},{"location":"contributing/#releasing-a-new-version","title":"Releasing a New Version","text":""},{"location":"contributing/#steps-for-creating-a-new-release","title":"Steps for Creating a New Release","text":"<ol> <li> <p>Ensure <code>main</code> is up to date:    Confirm all changes intended for the release are merged into the <code>main</code> branch.</p> </li> <li> <p>Update the version:    Manually bump the version number in <code>setup.py</code> based on the type of release (major, minor, or patch) following semantic versioning.</p> </li> <li> <p>Create a new tag:    Tag the release with the new version using the format <code>vX.X.X</code>. For example:    <pre><code>git tag v1.2.0\ngit push origin v1.2.0\n</code></pre></p> </li> <li> <p>Deploy to PyPI:    The GitHub Actions workflow will automatically build and deploy the package to PyPI once the tag is pushed.</p> </li> </ol>"},{"location":"contributing/#semantic-versioning-guidelines","title":"Semantic Versioning Guidelines","text":"<ul> <li>Major version: For incompatible API changes.</li> <li>Minor version: For backward-compatible features.</li> <li>Patch version: For backward-compatible bug fixes.</li> </ul>"},{"location":"contributing/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure compatibility with the latest dependencies.</li> <li>Update documentation when adding or changing features.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>To use PyForestScan, you need to have both PDAL (Point Data Abstraction Library) and GDAL (Geospatial Data Abstraction Library) installed. PDAL is a powerful library for working with point cloud data, and PyForestScan relies on it for many core functions.  GDAL contains the necessary raster processing capabilities and is used for writing geospatial raster data.</p>"},{"location":"installation/#gdal-and-pdal","title":"GDAL and PDAL","text":"<p>For complete installation guides, please visit the GDAL official documentation and PDAL official documentation sites. However, for a quick start, it is recommended to use conda to install both PDAL and GDAL, as it handles dependencies and installation paths efficiently. Once these dependencies are installed, you can then use pip to install pyforestscan within the same environment.</p> <p>Steps to install PDAL and GDAL:</p> <ol> <li>Install PDAL and GDAL using <code>conda</code>:</li> </ol> <pre><code>conda create -n pyforestscan_env -c conda-forge pdal gdal\n</code></pre> <ol> <li>Activate the conda env:</li> </ol> <pre><code>conda activate pyforestscan_env\n</code></pre> <p>For more information on PDAL and its capabilities, visit the official PDAL documentation: https://pdal.io/en/latest/.</p>"},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>Once dependencies are installed, PyForestScan can be installed from PyPI:</p> <pre><code>pip install pyforestscan\n</code></pre>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>It is also possible to install the latest development version from GitHub using Git:</p> <p>pip install git+https://github.com/iosefa/pyforestscan</p> <p>At any time, you can verify your installation:</p> <pre><code>pip show pyforestscan\n</code></pre>"},{"location":"installation/#docker","title":"Docker","text":"<p>A docker environment with <code>pyforestscan</code> is also available and includes jupyter with example notebooks and data. To use the docker environment:</p> <pre><code>docker run -it --rm -p 8888:8888 iosefa/pyforestscan:latest\n</code></pre> <p>This will launch a jupyter notebook. </p>"},{"location":"api/calculate/","title":"Calculate Module","text":""},{"location":"api/calculate/#pyforestscan.calculate.assign_voxels","title":"<code>assign_voxels(arr, voxel_resolution)</code>","text":"<p>Assigns voxel grids to spatial data points based on the specified resolutions.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input array-like object containing point cloud data with 'X', 'Y', and 'HeightAboveGround' fields.</p> required <code>voxel_resolution</code> <code>tuple of floats</code> <p>The resolution for x, y, and z dimensions of the voxel grid.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, List]</code> <p>tuple of (numpy.ndarray, List): A tuple containing the histogram of the voxel grid (with corrected orientation) and the extent of the point cloud.</p> Source code in <code>pyforestscan/calculate.py</code> <pre><code>def assign_voxels(arr, voxel_resolution) -&gt; Tuple[np.ndarray, List]:\n    \"\"\"\n    Assigns voxel grids to spatial data points based on the specified resolutions.\n\n    Args:\n        arr (numpy.ndarray): Input array-like object containing point cloud data with 'X', 'Y', and 'HeightAboveGround' fields.\n        voxel_resolution (tuple of floats): The resolution for x, y, and z dimensions of the voxel grid.\n\n    Returns:\n        tuple of (numpy.ndarray, List): A tuple containing the histogram of the voxel grid (with corrected orientation) and the extent of the point cloud.\n    \"\"\"\n    dx, dy, dz = voxel_resolution\n\n    pts = arr[arr['HeightAboveGround'] &gt;= 0]\n\n    x0 = np.floor(pts['X'].min() / dx) * dx\n    y0 = np.ceil (pts['Y'].max() / dy) * dy\n\n    x_bins = np.arange(x0, pts['X'].max() + dx, dx)\n    y_bins = np.arange(y0, pts['Y'].min() - dy, -dy)\n    z_bins = np.arange(0.0, pts['HeightAboveGround'].max() + dz, dz)\n\n    hist, _ = np.histogramdd(\n        np.column_stack((pts['X'], pts['Y'], pts['HeightAboveGround'])),\n        bins=(x_bins, y_bins[::-1], z_bins)\n    )\n    hist = hist[:, ::-1, :]\n\n    extent = [x_bins[0], x_bins[-1], y_bins[-1], y_bins[0]]\n    return hist, extent\n</code></pre>"},{"location":"api/calculate/#pyforestscan.calculate.calculate_canopy_cover","title":"<code>calculate_canopy_cover(pad, voxel_height, min_height=2.0, max_height=None, k=0.5)</code>","text":"<p>Calculate GEDI-style canopy cover at a height threshold using PAD.</p> <p>Uses the Beer\u2013Lambert relation: Cover(z) = 1 - exp(-k * PAI_above(z)), where PAI_above(z) is the integrated Plant Area Index above height z.</p> <p>Parameters:</p> Name Type Description Default <code>pad</code> <code>ndarray</code> <p>3D array of PAD values with shape (X, Y, Z).</p> required <code>voxel_height</code> <code>float</code> <p>Height of each voxel in meters (&gt; 0).</p> required <code>min_height</code> <code>float</code> <p>Height-above-ground threshold z (in meters) at which to compute canopy cover. Defaults to 2.0 m (GEDI convention).</p> <code>2.0</code> <code>max_height</code> <code>float or None</code> <p>Maximum height to integrate up to. If None, integrates to the top of the PAD volume. Defaults to None.</p> <code>None</code> <code>k</code> <code>float</code> <p>Extinction coefficient (Beer\u2013Lambert constant). Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 2D array (X, Y) of canopy cover values in [0, 1], with NaN where PAD is entirely missing for the integration range.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid (e.g., non-positive voxel_height, k &lt; 0, or min_height &gt;= max_height).</p> Source code in <code>pyforestscan/calculate.py</code> <pre><code>def calculate_canopy_cover(pad: np.ndarray,\n                           voxel_height: float,\n                           min_height: float = 2.0,\n                           max_height: float | None = None,\n                           k: float = 0.5) -&gt; np.ndarray:\n    \"\"\"\n    Calculate GEDI-style canopy cover at a height threshold using PAD.\n\n    Uses the Beer\u2013Lambert relation: Cover(z) = 1 - exp(-k * PAI_above(z)), where\n    PAI_above(z) is the integrated Plant Area Index above height z.\n\n    Args:\n        pad (np.ndarray): 3D array of PAD values with shape (X, Y, Z).\n        voxel_height (float): Height of each voxel in meters (&gt; 0).\n        min_height (float, optional): Height-above-ground threshold z (in meters) at which\n            to compute canopy cover. Defaults to 2.0 m (GEDI convention).\n        max_height (float or None, optional): Maximum height to integrate up to. If None,\n            integrates to the top of the PAD volume. Defaults to None.\n        k (float, optional): Extinction coefficient (Beer\u2013Lambert constant). Defaults to 0.5.\n\n    Returns:\n        np.ndarray: 2D array (X, Y) of canopy cover values in [0, 1], with NaN where\n            PAD is entirely missing for the integration range.\n\n    Raises:\n        ValueError: If parameters are invalid (e.g., non-positive voxel_height, k &lt; 0,\n            or min_height &gt;= max_height).\n    \"\"\"\n    if voxel_height &lt;= 0:\n        raise ValueError(f\"voxel_height must be &gt; 0 metres (got {voxel_height})\")\n    if k &lt; 0:\n        raise ValueError(f\"k must be &gt;= 0 (got {k})\")\n\n    # Compute PAI integrated from min_height up to max_height/top\n    pai_above = calculate_pai(pad, voxel_height, min_height=min_height, max_height=max_height)\n\n    # Identify columns that are entirely NaN within the integration range\n    if max_height is None:\n        max_height = pad.shape[2] * voxel_height\n    if min_height &gt;= max_height:\n        raise ValueError(\"Minimum height index must be less than maximum height index.\")\n    start_idx = int(np.ceil(min_height / voxel_height))\n    end_idx = int(np.floor(max_height / voxel_height))\n    range_slice = pad[:, :, start_idx:end_idx]\n    all_nan_mask = np.all(np.isnan(range_slice), axis=2)\n\n    # Beer\u2013Lambert canopy cover\n    cover = 1.0 - np.exp(-k * pai_above)\n\n    # Clamp to [0,1] and set invalids\n    cover = np.where(np.isfinite(cover), cover, np.nan)\n    cover = np.clip(cover, 0.0, 1.0)\n    cover[all_nan_mask] = np.nan\n    return cover\n</code></pre>"},{"location":"api/calculate/#pyforestscan.calculate.calculate_chm","title":"<code>calculate_chm(arr, voxel_resolution, interpolation='linear', interp_valid_region=False, interp_clean_edges=False)</code>","text":"<p>Calculate the Canopy Height Model (CHM) for a given voxel grid.</p> <p>The CHM is computed as the maximum 'HeightAboveGround' value within each (X, Y) voxel. Optionally, gaps in the CHM can be filled using interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input structured numpy array containing point cloud data with fields 'X', 'Y', and 'HeightAboveGround'.</p> required <code>voxel_resolution</code> <code>tuple of float</code> <p>The resolution for the X and Y dimensions of the voxel grid, specified as (x_resolution, y_resolution).</p> required <code>interpolation</code> <code>str or None</code> <p>Method for interpolating gaps in the CHM. Supported methods are \"nearest\", \"linear\", \"cubic\", or None. If None, no interpolation is performed. Defaults to \"linear\".</p> <code>'linear'</code> <code>interp_valid_region</code> <code>bool</code> <p>Whether to calculate a valid region mask using morphological operations for interpolation. If True, interpolation is only applied within the valid data region. If False (default), interpolation is applied to all NaN values. Ignored if <code>interpolation</code> is None.</p> <code>False</code> <code>interp_clean_edges</code> <code>bool</code> <p>Whether to clean edge fringes of the interpolated CHM. Default is False. Ignored if <code>interpolation</code> is None.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, List]</code> <ul> <li>np.ndarray: 2D numpy array representing the CHM, with each value corresponding to the maximum     height in that (X, Y) voxel.</li> <li>list: The spatial extent as [x_min, x_max, y_min, y_max].</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input array does not contain the required fields.</p> <code>ValueError</code> <p>If <code>interpolation</code> is specified but not one of the supported methods.</p> Source code in <code>pyforestscan/calculate.py</code> <pre><code>def calculate_chm(arr, voxel_resolution, interpolation=\"linear\",\n                  interp_valid_region=False, interp_clean_edges=False) -&gt; Tuple[np.ndarray, List]:\n    \"\"\"\n    Calculate the Canopy Height Model (CHM) for a given voxel grid.\n\n    The CHM is computed as the maximum 'HeightAboveGround' value within each (X, Y) voxel.\n    Optionally, gaps in the CHM can be filled using interpolation.\n\n    Args:\n        arr (np.ndarray): Input structured numpy array containing point cloud data\n            with fields 'X', 'Y', and 'HeightAboveGround'.\n        voxel_resolution (tuple of float): The resolution for the X and Y dimensions\n            of the voxel grid, specified as (x_resolution, y_resolution).\n        interpolation (str or None, optional): Method for interpolating gaps in the CHM.\n            Supported methods are \"nearest\", \"linear\", \"cubic\", or None. If None, no interpolation\n            is performed. Defaults to \"linear\".\n        interp_valid_region (bool): Whether to calculate a valid region mask using morphological operations for\n            interpolation. If True, interpolation is only applied within the valid data region. If False (default),\n            interpolation is applied to all NaN values. Ignored if `interpolation` is None.\n        interp_clean_edges (bool): Whether to clean edge fringes of the interpolated CHM. Default is False.\n            Ignored if `interpolation` is None.\n\n    Returns:\n        tuple:\n            - np.ndarray: 2D numpy array representing the CHM, with each value corresponding to the maximum\n                height in that (X, Y) voxel.\n            - list: The spatial extent as [x_min, x_max, y_min, y_max].\n\n    Raises:\n        ValueError: If input array does not contain the required fields.\n        ValueError: If `interpolation` is specified but not one of the supported methods.\n\n    \"\"\"\n    x_resolution, y_resolution = voxel_resolution[:2]\n    x = arr['X']\n    y = arr['Y']\n    z = arr['HeightAboveGround']\n\n    x_min, x_max = x.min(), x.max()\n    y_min, y_max = y.min(), y.max()\n\n    x_bins = np.arange(x_min, x_max + x_resolution, x_resolution)\n    y_bins = np.arange(y_min, y_max + y_resolution, y_resolution)\n\n    x_indices = np.digitize(x, x_bins) - 1\n    y_indices = np.digitize(y, y_bins) - 1\n\n    chm = np.full((len(x_bins) - 1, len(y_bins) - 1), np.nan)\n\n    for xi, yi, zi in zip(x_indices, y_indices, z):\n        if 0 &lt;= xi &lt; chm.shape[0] and 0 &lt;= yi &lt; chm.shape[1]:\n            if np.isnan(chm[xi, yi]) or zi &gt; chm[xi, yi]:\n                chm[xi, yi] = zi\n\n    if interpolation is not None:\n        if interp_valid_region is True:\n            valid_region_mask = _calc_valid_region_mask(chm)\n            interp_mask = np.isnan(chm) &amp; valid_region_mask\n        else:\n            interp_mask = np.isnan(chm)\n\n        if np.any(interp_mask):\n            x_grid, y_grid = np.meshgrid(\n                (x_bins[:-1] + x_bins[1:]) / 2,\n                (y_bins[:-1] + y_bins[1:]) / 2\n            )\n\n            valid_mask = ~np.isnan(chm)\n            valid_x = x_grid.flatten()[valid_mask.flatten()]\n            valid_y = y_grid.flatten()[valid_mask.flatten()]\n            valid_values = chm.flatten()[valid_mask.flatten()]\n\n            interp_coords = np.column_stack([\n                x_grid.flatten()[interp_mask.flatten()],\n                y_grid.flatten()[interp_mask.flatten()]\n            ])\n\n            if len(interp_coords) &gt; 0 and len(valid_values) &gt; 0:\n                chm[interp_mask] = griddata(\n                    points=np.column_stack([valid_x, valid_y]),\n                    values=valid_values,\n                    xi=interp_coords,\n                    method=interpolation\n                )\n            if interp_clean_edges:\n                chm = _clean_edges(chm)\n\n    chm = np.flip(chm, axis=1)\n    extent = [x_min, x_max, y_min, y_max]\n\n    return chm, extent\n</code></pre>"},{"location":"api/calculate/#pyforestscan.calculate.calculate_fhd","title":"<code>calculate_fhd(voxel_returns)</code>","text":"<p>Calculate the Foliage Height Diversity (FHD) for a given set of voxel returns.</p> <p>This function computes FHD by calculating the entropy of the voxel return proportions along the Z (height) axis, which represents the vertical diversity of canopy structure.</p> <p>Parameters:</p> Name Type Description Default <code>voxel_returns</code> <code>ndarray</code> <p>3D numpy array of shape (X, Y, Z) representing voxel returns, where X and Y are spatial dimensions and Z represents height bins (vertical layers).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 2D numpy array of shape (X, Y) with FHD values for each (X, Y) location. Areas with no voxel returns will have NaN values.</p> Source code in <code>pyforestscan/calculate.py</code> <pre><code>def calculate_fhd(voxel_returns) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the Foliage Height Diversity (FHD) for a given set of voxel returns.\n\n    This function computes FHD by calculating the entropy of the voxel return proportions\n    along the Z (height) axis, which represents the vertical diversity of canopy structure.\n\n    Args:\n        voxel_returns (np.ndarray): 3D numpy array of shape (X, Y, Z) representing voxel returns,\n            where X and Y are spatial dimensions and Z represents height bins (vertical layers).\n\n    Returns:\n        np.ndarray: 2D numpy array of shape (X, Y) with FHD values for each (X, Y) location.\n            Areas with no voxel returns will have NaN values.\n    \"\"\"\n    sum_counts = np.sum(voxel_returns, axis=2)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        proportions = np.divide(\n            voxel_returns,\n            sum_counts[..., None],\n            out=np.zeros_like(voxel_returns, dtype=float),\n            where=sum_counts[..., None] != 0\n        )\n\n    fhd = entropy(proportions, axis=2)\n    fhd[sum_counts == 0] = np.nan\n    return fhd\n</code></pre>"},{"location":"api/calculate/#pyforestscan.calculate.calculate_pad","title":"<code>calculate_pad(voxel_returns, voxel_height=1.0, beer_lambert_constant=1.0, drop_ground=True)</code>","text":"<p>Calculate the Plant Area Density (PAD) using the Beer-Lambert Law.</p> <p>Parameters:</p> Name Type Description Default <code>voxel_returns</code> <code>ndarray</code> <p>3D numpy array of shape (X, Y, Z) representing the LiDAR returns in each voxel column.</p> required <code>voxel_height</code> <code>float</code> <p>Height of each voxel. Defaults to 1.0.</p> <code>1.0</code> <code>beer_lambert_constant</code> <code>float</code> <p>The Beer-Lambert constant used in the calculation. Defaults to 1.0.</p> <code>1.0</code> <code>drop_ground</code> <code>bool</code> <p>If True, sets PAD values in the ground (lowest) voxel layer to NaN in the output. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 3D numpy array containing PAD values for each voxel, same shape as <code>voxel_returns</code>. Columns that have zero returns across all Z are set to NaN.</p> Source code in <code>pyforestscan/calculate.py</code> <pre><code>def calculate_pad(voxel_returns,\n                  voxel_height=1.0,\n                  beer_lambert_constant=1.0,\n                  drop_ground=True\n                  ) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the Plant Area Density (PAD) using the Beer-Lambert Law.\n\n    Args:\n        voxel_returns (np.ndarray): 3D numpy array of shape (X, Y, Z) representing\n            the LiDAR returns in each voxel column.\n        voxel_height (float, optional): Height of each voxel. Defaults to 1.0.\n        beer_lambert_constant (float, optional): The Beer-Lambert constant used\n            in the calculation. Defaults to 1.0.\n        drop_ground (bool, optional): If True, sets PAD values in the ground (lowest)\n            voxel layer to NaN in the output. Defaults to True.\n\n    Returns:\n        np.ndarray: 3D numpy array containing PAD values for each voxel, same shape as `voxel_returns`.\n            Columns that have zero returns across all Z are set to NaN.\n    \"\"\"\n    if voxel_height &lt;= 0:\n        raise ValueError(\n            f\"voxel_height must be &gt; 0 metres (got {voxel_height})\"\n        )\n    reversed_cols = voxel_returns[:, :, ::-1]\n\n    total = np.sum(reversed_cols, axis=2, keepdims=True)\n\n    csum = np.cumsum(reversed_cols, axis=2)\n\n    shots_out = total - csum\n\n    shots_in = np.concatenate(\n        (total, shots_out[:, :, :-1]), axis=2\n    )\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        pad_sky = np.log(shots_in / shots_out) / (beer_lambert_constant * voxel_height)\n    pad_sky[~np.isfinite(pad_sky)] = np.nan\n\n    pad = pad_sky[:, :, ::-1]\n\n    if drop_ground:\n        pad[:, :, 0] = np.nan\n\n    # Mask only columns that have zero returns across all Z (true empty columns)\n    empty_columns = (np.sum(voxel_returns, axis=2) == 0)\n    pad[empty_columns, :] = np.nan\n\n    return pad\n</code></pre>"},{"location":"api/calculate/#pyforestscan.calculate.calculate_pai","title":"<code>calculate_pai(pad, voxel_height, min_height=1.0, max_height=None)</code>","text":"<p>Calculate Plant Area Index (PAI) from Plant Area Density (PAD) data by summing PAD values along the height (Z) axis.</p> <p>Parameters:</p> Name Type Description Default <code>pad</code> <code>ndarray</code> <p>3D numpy array representing Plant Area Density (PAD) values, shape (X, Y, Z).</p> required <code>voxel_height</code> <code>float</code> <p>Height of each voxel in meters.</p> required <code>min_height</code> <code>float</code> <p>Minimum height in meters for summing PAD values. Defaults to 1.0.</p> <code>1.0</code> <code>max_height</code> <code>float</code> <p>Maximum height in meters for summing PAD values. If None, uses the full height of the input array. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 2D numpy array of shape (X, Y) with PAI values for each (x, y) voxel column.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If min_height is greater than or equal to max_height.</p> Source code in <code>pyforestscan/calculate.py</code> <pre><code>def calculate_pai(pad,\n                  voxel_height,\n                  min_height=1.0,\n                  max_height=None) -&gt; np.ndarray:\n    \"\"\"\n    Calculate Plant Area Index (PAI) from Plant Area Density (PAD) data by summing PAD values along the height (Z) axis.\n\n    Args:\n        pad (np.ndarray): 3D numpy array representing Plant Area Density (PAD) values, shape (X, Y, Z).\n        voxel_height (float): Height of each voxel in meters.\n        min_height (float, optional): Minimum height in meters for summing PAD values. Defaults to 1.0.\n        max_height (float, optional): Maximum height in meters for summing PAD values. If None, uses the full height of the input array. Defaults to None.\n\n    Returns:\n        np.ndarray: 2D numpy array of shape (X, Y) with PAI values for each (x, y) voxel column.\n\n    Raises:\n        ValueError: If min_height is greater than or equal to max_height.\n    \"\"\"\n    if max_height is None:\n        max_height = pad.shape[2] * voxel_height\n\n    if min_height &gt;= max_height:\n        raise ValueError(\"Minimum height index must be less than maximum height index.\")\n\n    start_idx = int(np.ceil(min_height / voxel_height))\n    end_idx   = int(np.floor(max_height / voxel_height))\n\n    core = pad[:, :, start_idx:end_idx]\n    pai  = np.nansum(core, axis=2) * voxel_height\n    return pai\n</code></pre>"},{"location":"api/calculate/#pyforestscan.calculate.generate_dtm","title":"<code>generate_dtm(ground_points, resolution=2.0)</code>","text":"<p>Generates a Digital Terrain Model (DTM) raster from classified ground points.</p> <p>Parameters:</p> Name Type Description Default <code>ground_points</code> <code>list</code> <p>Point cloud arrays of classified ground points.</p> required <code>resolution</code> <code>float</code> <p>Spatial resolution of the DTM in meters.</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, List]</code> <p>A tuple containing the DTM as a 2D NumPy array and the spatial extent [x_min, x_max, y_min, y_max].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no ground points are found for DTM generation.</p> <code>KeyError</code> <p>If point cloud data is missing 'X', 'Y', or 'Z' fields.</p> Source code in <code>pyforestscan/calculate.py</code> <pre><code>def generate_dtm(ground_points, resolution=2.0) -&gt; Tuple[np.ndarray, List]:\n    \"\"\"\n    Generates a Digital Terrain Model (DTM) raster from classified ground points.\n\n    Args:\n        ground_points (list): Point cloud arrays of classified ground points.\n        resolution (float): Spatial resolution of the DTM in meters.\n\n    Returns:\n        tuple: A tuple containing the DTM as a 2D NumPy array and the spatial extent [x_min, x_max, y_min, y_max].\n\n    Raises:\n        ValueError: If no ground points are found for DTM generation.\n        KeyError: If point cloud data is missing 'X', 'Y', or 'Z' fields.\n    \"\"\"\n    #todo: add parameter to allow interpolation of NA values.\n    try:\n        x = np.array([pt['X'] for array in ground_points for pt in array])\n        y = np.array([pt['Y'] for array in ground_points for pt in array])\n        z = np.array([pt['Z'] for array in ground_points for pt in array])\n    except ValueError:\n        raise ValueError(\"Ground point cloud data missing 'X', 'Y', or 'Z' fields.\")\n\n    x_min, x_max = x.min(), x.max()\n    y_min, y_max = y.min(), y.max()\n\n    x_bins = np.arange(x_min, x_max + resolution, resolution)\n    y_bins = np.arange(y_min, y_max + resolution, resolution)\n\n    x_indices = np.digitize(x, x_bins) - 1\n    y_indices = np.digitize(y, y_bins) - 1\n\n    dtm = np.full((len(x_bins) - 1, len(y_bins) - 1), np.nan)\n\n    for xi, yi, zi in zip(x_indices, y_indices, z):\n        if 0 &lt;= xi &lt; dtm.shape[0] and 0 &lt;= yi &lt; dtm.shape[1]:\n            if np.isnan(dtm[xi, yi]) or zi &lt; dtm[xi, yi]:\n                dtm[xi, yi] = zi\n\n    dtm = np.fliplr(dtm)\n\n    extent = [x_min, x_max, y_min, y_max]\n\n    return dtm, extent\n</code></pre>"},{"location":"api/filters/","title":"Filters Module","text":""},{"location":"api/filters/#pyforestscan.filters.classify_ground_points","title":"<code>classify_ground_points(arrays, ignore_class='Classification[7:7]', cell=1.0, cut=0.0, returns='last,only', scalar=1.25, slope=0.15, threshold=0.5, window=18.0)</code>","text":"<p>Apply the SMRF (Simple Morphological Filter) to classify ground points in the point cloud arrays.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list</code> <p>Cleaned point cloud arrays after outlier removal, typically as a list of structured arrays.</p> required <code>ignore_class</code> <code>str</code> <p>Classification codes to ignore during filtering, e.g., \"Classification[7:7]\". Defaults to \"Classification[7:7]\".</p> <code>'Classification[7:7]'</code> <code>cell</code> <code>float</code> <p>Cell size in meters for the morphological filter grid. Defaults to 1.0.</p> <code>1.0</code> <code>cut</code> <code>float</code> <p>Cut net size; if set to 0, net cutting is skipped. Defaults to 0.0.</p> <code>0.0</code> <code>returns</code> <code>str</code> <p>Return types to include in output. Supported values include \"first\", \"last\", \"intermediate\", \"only\". Defaults to \"last,only\".</p> <code>'last,only'</code> <code>scalar</code> <code>float</code> <p>Elevation scalar for filter sensitivity. Defaults to 1.25.</p> <code>1.25</code> <code>slope</code> <code>float</code> <p>Slope threshold for ground classification. Defaults to 0.15.</p> <code>0.15</code> <code>threshold</code> <code>float</code> <p>Elevation threshold for morphological operations. Defaults to 0.5.</p> <code>0.5</code> <code>window</code> <code>float</code> <p>Maximum window size in meters for filter application. Defaults to 18.0.</p> <code>18.0</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List</code> <p>Point cloud arrays with classified ground points.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an error during pipeline execution.</p> <code>ValueError</code> <p>If no data is returned after SMRF classification.</p> Source code in <code>pyforestscan/filters.py</code> <pre><code>def classify_ground_points(\n        arrays,\n        ignore_class=\"Classification[7:7]\",\n        cell=1.0,\n        cut=0.0,\n        returns=\"last,only\",\n        scalar=1.25,\n        slope=0.15,\n        threshold=0.5,\n        window=18.0\n) -&gt; List:\n    \"\"\"\n    Apply the SMRF (Simple Morphological Filter) to classify ground points in the point cloud arrays.\n\n    Args:\n        arrays (list): Cleaned point cloud arrays after outlier removal, typically as a list\n            of structured arrays.\n        ignore_class (str, optional): Classification codes to ignore during filtering,\n            e.g., \"Classification[7:7]\". Defaults to \"Classification[7:7]\".\n        cell (float, optional): Cell size in meters for the morphological filter grid. Defaults to 1.0.\n        cut (float, optional): Cut net size; if set to 0, net cutting is skipped. Defaults to 0.0.\n        returns (str, optional): Return types to include in output. Supported values include\n            \"first\", \"last\", \"intermediate\", \"only\". Defaults to \"last,only\".\n        scalar (float, optional): Elevation scalar for filter sensitivity. Defaults to 1.25.\n        slope (float, optional): Slope threshold for ground classification. Defaults to 0.15.\n        threshold (float, optional): Elevation threshold for morphological operations. Defaults to 0.5.\n        window (float, optional): Maximum window size in meters for filter application. Defaults to 18.0.\n\n    Returns:\n        list: Point cloud arrays with classified ground points.\n\n    Raises:\n        RuntimeError: If there is an error during pipeline execution.\n        ValueError: If no data is returned after SMRF classification.\n    \"\"\"\n    pipeline_stages = [\n        _filter_smrf(\n            cell=cell,\n            cut=cut,\n            ignore=ignore_class,\n            returns=returns,\n            scalar=scalar,\n            slope=slope,\n            threshold=threshold,\n            window=window\n        )\n    ]\n\n    try:\n        pipeline = _build_pdal_pipeline(arrays, pipeline_stages)\n    except RuntimeError as e:\n        raise RuntimeError(f\"SMRF Classification Pipeline Failed: {e}\")\n\n    processed_arrays = pipeline.arrays\n\n    if not processed_arrays:\n        raise ValueError(\"No data returned after SMRF classification.\")\n\n    return processed_arrays\n</code></pre>"},{"location":"api/filters/#pyforestscan.filters.downsample_poisson","title":"<code>downsample_poisson(arrays, thin_radius)</code>","text":"<p>Downsample point cloud arrays using Poisson (radius-based) thinning.</p> <p>This function applies a radius-based filter to the input point cloud arrays, reducing the point density so that no two points are closer than the specified radius.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list</code> <p>List of point cloud arrays to be downsampled.</p> required <code>thin_radius</code> <code>float</code> <p>Minimum allowed distance (radius) between retained points, in the same units as the point cloud coordinates.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List</code> <p>List of downsampled point cloud arrays.</p> Source code in <code>pyforestscan/filters.py</code> <pre><code>def downsample_poisson(arrays, thin_radius) -&gt; List:\n    \"\"\"\n    Downsample point cloud arrays using Poisson (radius-based) thinning.\n\n    This function applies a radius-based filter to the input point cloud arrays,\n    reducing the point density so that no two points are closer than the specified radius.\n\n    Args:\n        arrays (list): List of point cloud arrays to be downsampled.\n        thin_radius (float): Minimum allowed distance (radius) between retained points, in the same units as the point cloud coordinates.\n\n    Returns:\n        list: List of downsampled point cloud arrays.\n    \"\"\"\n    pipeline = _build_pdal_pipeline(arrays, [_filter_radius(thin_radius)])\n    return pipeline.arrays\n</code></pre>"},{"location":"api/filters/#pyforestscan.filters.filter_ground","title":"<code>filter_ground(arrays)</code>","text":"<p>Remove ground points (classification 2) from a list of point cloud arrays.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list</code> <p>List of point cloud arrays to be processed.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List</code> <p>List of point cloud arrays after removing points classified as ground (classification 2).</p> Source code in <code>pyforestscan/filters.py</code> <pre><code>def filter_ground(arrays) -&gt; List:\n    \"\"\"\n    Remove ground points (classification 2) from a list of point cloud arrays.\n\n    Args:\n        arrays (list): List of point cloud arrays to be processed.\n\n    Returns:\n        list: List of point cloud arrays after removing points classified as ground (classification 2).\n    \"\"\"\n    pipeline = _build_pdal_pipeline(arrays, [_filter_ground()])\n    return pipeline.arrays\n</code></pre>"},{"location":"api/filters/#pyforestscan.filters.filter_hag","title":"<code>filter_hag(arrays, lower_limit=0, upper_limit=None)</code>","text":"<p>Apply a Height Above Ground (HAG) filter to a list of point cloud arrays.</p> <p>Filters each input array to include only points where Height Above Ground is within the specified range.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list</code> <p>List of point cloud arrays to be processed.</p> required <code>lower_limit</code> <code>int or float</code> <p>Minimum Height Above Ground value to keep. Defaults to 0.</p> <code>0</code> <code>upper_limit</code> <code>int or float</code> <p>Maximum Height Above Ground value to keep. If None, no upper limit is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List</code> <p>List of processed point cloud arrays after applying the HAG filter.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If upper_limit is specified and is less than lower_limit.</p> Source code in <code>pyforestscan/filters.py</code> <pre><code>def filter_hag(arrays, lower_limit=0, upper_limit=None) -&gt; List:\n    \"\"\"\n    Apply a Height Above Ground (HAG) filter to a list of point cloud arrays.\n\n    Filters each input array to include only points where Height Above Ground is\n    within the specified range.\n\n    Args:\n        arrays (list): List of point cloud arrays to be processed.\n        lower_limit (int or float, optional): Minimum Height Above Ground value to keep.\n            Defaults to 0.\n        upper_limit (int or float, optional): Maximum Height Above Ground value to keep.\n            If None, no upper limit is applied. Defaults to None.\n\n    Returns:\n        list: List of processed point cloud arrays after applying the HAG filter.\n\n    Raises:\n        ValueError: If upper_limit is specified and is less than lower_limit.\n    \"\"\"\n    pipeline = _build_pdal_pipeline(arrays, [_filter_hag(lower_limit, upper_limit)])\n    return pipeline.arrays\n</code></pre>"},{"location":"api/filters/#pyforestscan.filters.filter_select_ground","title":"<code>filter_select_ground(arrays)</code>","text":"<p>Select only ground points (classification 2) from a list of point cloud arrays.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list</code> <p>List of point cloud arrays to be processed.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List</code> <p>List of point cloud arrays containing only points classified as ground (classification 2).</p> Source code in <code>pyforestscan/filters.py</code> <pre><code>def filter_select_ground(arrays) -&gt; List:\n    \"\"\"\n    Select only ground points (classification 2) from a list of point cloud arrays.\n\n    Args:\n        arrays (list): List of point cloud arrays to be processed.\n\n    Returns:\n        list: List of point cloud arrays containing only points classified as ground (classification 2).\n    \"\"\"\n    pipeline = _build_pdal_pipeline(arrays, [_select_ground()])\n    return pipeline.arrays\n</code></pre>"},{"location":"api/filters/#pyforestscan.filters.remove_outliers_and_clean","title":"<code>remove_outliers_and_clean(arrays, mean_k=8, multiplier=3.0, remove=False)</code>","text":"<p>Processes input arrays by removing statistical outliers and optionally cleans the data, filtering out specific classifications. By default, this function only labels outliers as \"7\" (outlier).</p>"},{"location":"api/filters/#pyforestscan.filters.remove_outliers_and_clean--todo-this-function-will-be-renamed-in-a-future-release","title":"todo: this function will be renamed in a future release.","text":"<p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list</code> <p>Input arrays to process, typically a list of structured arrays or similar data types (e.g., numpy structured arrays with a 'Classification' field).</p> required <code>mean_k</code> <code>int</code> <p>Number of neighbors to analyze for each point during statistical outlier removal. Defaults to 8.</p> <code>8</code> <code>multiplier</code> <code>float</code> <p>Multiplication factor for the standard deviation threshold to classify a point as an outlier. Defaults to 3.0.</p> <code>3.0</code> <code>remove</code> <code>bool</code> <p>If True, remove points classified as outliers (Classification == 7) after labeling. If False, only label outliers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List</code> <p>List of processed arrays after outlier removal and, if specified, cleaning.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the outlier removal pipeline fails during execution.</p> <code>ValueError</code> <p>If no data is returned after applying outlier removal.</p> Source code in <code>pyforestscan/filters.py</code> <pre><code>def remove_outliers_and_clean(arrays, mean_k=8, multiplier=3.0, remove=False) -&gt; List:\n    \"\"\"\n    Processes input arrays by removing statistical outliers and optionally cleans\n    the data, filtering out specific classifications.\n    By default, this function only labels outliers as \"7\" (outlier).\n\n    # todo: this function will be renamed in a future release.\n\n    Args:\n        arrays (list): Input arrays to process, typically a list of structured arrays or\n            similar data types (e.g., numpy structured arrays with a 'Classification' field).\n        mean_k (int, optional): Number of neighbors to analyze for each point during\n            statistical outlier removal. Defaults to 8.\n        multiplier (float, optional): Multiplication factor for the standard deviation\n            threshold to classify a point as an outlier. Defaults to 3.0.\n        remove (bool, optional): If True, remove points classified as outliers (Classification == 7)\n            after labeling. If False, only label outliers. Defaults to False.\n\n    Returns:\n        list: List of processed arrays after outlier removal and, if specified, cleaning.\n\n    Raises:\n        RuntimeError: If the outlier removal pipeline fails during execution.\n        ValueError: If no data is returned after applying outlier removal.\n    \"\"\"\n    pipeline_stages = [\n        _filter_statistical_outlier(mean_k=mean_k, multiplier=multiplier)\n    ]\n\n    try:\n        pipeline = _build_pdal_pipeline(arrays, pipeline_stages)\n    except RuntimeError as e:\n        raise RuntimeError(f\"Outlier Removal Pipeline Failed: {e}\")\n\n    if remove:\n        clean_arrays = []\n        for arr in pipeline.arrays:\n            mask = (arr[\"Classification\"] != 7)\n            cleaned = arr[mask]\n            clean_arrays.append(cleaned)\n        processed_arrays = clean_arrays\n    else:\n        processed_arrays = pipeline.arrays\n\n    if not processed_arrays:\n        raise ValueError(\"No data returned after outlier removal.\")\n\n    return processed_arrays\n</code></pre>"},{"location":"api/handlers/","title":"Handlers Module","text":""},{"location":"api/handlers/#pyforestscan.handlers.create_geotiff","title":"<code>create_geotiff(layer, output_file, crs, spatial_extent, nodata=-9999)</code>","text":"<p>Create a GeoTIFF file from the given data layer.</p> <p>The function transposes the input layer before saving and writes it as a single-band GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>ndarray</code> <p>The data layer to write, assumed to have shape (X, Y).</p> required <code>output_file</code> <code>str</code> <p>Path where the GeoTIFF file will be saved.</p> required <code>crs</code> <code>str</code> <p>Coordinate Reference System for the GeoTIFF.</p> required <code>spatial_extent</code> <code>tuple</code> <p>The spatial extent as (x_min, x_max, y_min, y_max).</p> required <code>nodata</code> <code>float or int</code> <p>Value to use for NoData areas. Defaults to -9999.</p> <code>-9999</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>RasterioError</code> <p>If there is an error creating the GeoTIFF.</p> <code>ValueError</code> <p>If the layer has invalid dimensions or the spatial extent is invalid.</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def create_geotiff(layer, output_file, crs, spatial_extent, nodata=-9999) -&gt; None:\n    \"\"\"\n    Create a GeoTIFF file from the given data layer.\n\n    The function transposes the input layer before saving and writes it as a single-band GeoTIFF.\n\n    Args:\n        layer (np.ndarray): The data layer to write, assumed to have shape (X, Y).\n        output_file (str): Path where the GeoTIFF file will be saved.\n        crs (str): Coordinate Reference System for the GeoTIFF.\n        spatial_extent (tuple): The spatial extent as (x_min, x_max, y_min, y_max).\n        nodata (float or int, optional): Value to use for NoData areas. Defaults to -9999.\n\n    Returns:\n        None\n\n    Raises:\n        rasterio.errors.RasterioError: If there is an error creating the GeoTIFF.\n        ValueError: If the layer has invalid dimensions or the spatial extent is invalid.\n    \"\"\"\n    layer = np.nan_to_num(layer, nan=-9999)\n\n    x_min, x_max, y_min, y_max = spatial_extent\n\n    if layer.size == 0 or layer.shape[0] == 0 or layer.shape[1] == 0:\n        raise ValueError(f\"Invalid layer dimensions: {layer.shape}. Cannot create GeoTIFF.\")\n\n    if x_max &lt;= x_min or y_max &lt;= y_min:\n        raise ValueError(f\"Invalid spatial extent: {spatial_extent}.\")\n\n    layer = layer.T\n\n    transform = from_bounds(\n        x_min, y_min, x_max, y_max,\n        layer.shape[1], layer.shape[0]\n    )\n\n    with rasterio.open(\n            output_file, 'w',\n            driver='GTiff',\n            height=layer.shape[0],\n            width=layer.shape[1],\n            count=1,\n            dtype=layer.dtype.name,\n            crs=crs,\n            transform=transform,\n            nodata=nodata\n    ) as new_dataset:\n        new_dataset.write(layer, 1)\n</code></pre>"},{"location":"api/handlers/#pyforestscan.handlers.get_raster_epsg","title":"<code>get_raster_epsg(dtm_path)</code>","text":"<p>Retrieve the EPSG code from a raster file.</p> <p>Parameters:</p> Name Type Description Default <code>dtm_path</code> <code>str</code> <p>File path to the raster file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The EPSG code or CRS string of the raster file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def get_raster_epsg(dtm_path) -&gt; str:\n    \"\"\"\n    Retrieve the EPSG code from a raster file.\n\n    Args:\n        dtm_path (str): File path to the raster file.\n\n    Returns:\n        str: The EPSG code or CRS string of the raster file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n    \"\"\"\n    if not os.path.isfile(dtm_path):\n        raise FileNotFoundError(f\"No such file: '{dtm_path}'\")\n    with rasterio.open(dtm_path) as dtm:\n        return dtm.crs.to_string()\n</code></pre>"},{"location":"api/handlers/#pyforestscan.handlers.load_polygon_from_file","title":"<code>load_polygon_from_file(vector_file_path, index=0)</code>","text":"<p>Load a polygon geometry and its CRS from a given vector file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_file_path</code> <code>str</code> <p>Path to the vector file containing the polygon geometry.</p> required <code>index</code> <code>int</code> <p>Index of the polygon to load from the file. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, str]</code> <p>A tuple containing: - str: Well-Known Text (WKT) representation of the selected polygon. - str: Coordinate reference system (CRS) of the vector file as a string.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified vector file does not exist.</p> <code>ValueError</code> <p>If the file cannot be read or is not a valid vector file format.</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def load_polygon_from_file(vector_file_path, index=0) -&gt; Tuple[str, str]:\n    \"\"\"\n    Load a polygon geometry and its CRS from a given vector file.\n\n    Args:\n        vector_file_path (str): Path to the vector file containing the polygon geometry.\n        index (int, optional): Index of the polygon to load from the file. Defaults to 0.\n\n    Returns:\n        tuple: A tuple containing:\n            - str: Well-Known Text (WKT) representation of the selected polygon.\n            - str: Coordinate reference system (CRS) of the vector file as a string.\n\n    Raises:\n        FileNotFoundError: If the specified vector file does not exist.\n        ValueError: If the file cannot be read or is not a valid vector file format.\n    \"\"\"\n    if not os.path.isfile(vector_file_path):\n        raise FileNotFoundError(f\"No such file: '{vector_file_path}'\")\n\n    try:\n        gdf = gpd.read_file(vector_file_path)\n    except Exception as e:\n        raise ValueError(f\"Unable to read file: {vector_file_path}. Ensure it is a valid vector file format.\") from e\n\n    polygon = gdf.loc[index, 'geometry']\n    if isinstance(polygon, MultiPolygon):\n        polygon = list(polygon.geoms)[0]\n    return polygon.wkt, gdf.crs.to_string()\n</code></pre>"},{"location":"api/handlers/#pyforestscan.handlers.read_lidar","title":"<code>read_lidar(input_file, srs, bounds=None, thin_radius=None, hag=False, hag_dtm=False, dtm=None, crop_poly=False, poly=None)</code>","text":"<p>Read and process a LiDAR point cloud file using PDAL with various options.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input LiDAR file. Supported formats: .las, .laz, .copc, .copc.laz, or ept.json.</p> required <code>srs</code> <code>str</code> <p>Spatial Reference System (SRS) of the point cloud.</p> required <code>bounds</code> <code>tuple or list</code> <p>Bounds for cropping data (only applies to EPT format). Format: ([xmin, xmax], [ymin, ymax], [zmin, zmax]).</p> <code>None</code> <code>thin_radius</code> <code>float</code> <p>Radius for thinning the point cloud. Must be positive.</p> <code>None</code> <code>hag</code> <code>bool</code> <p>If True, calculate Height Above Ground (HAG) using Delaunay triangulation. Defaults to False.</p> <code>False</code> <code>hag_dtm</code> <code>bool</code> <p>If True, calculate Height Above Ground (HAG) using a DTM file. Defaults to False.</p> <code>False</code> <code>dtm</code> <code>str</code> <p>Path to the DTM (.tif) file, required if hag_dtm is True.</p> <code>None</code> <code>crop_poly</code> <code>bool</code> <p>If True, crop the point cloud using a polygon. Defaults to False.</p> <code>False</code> <code>poly</code> <code>str</code> <p>Path to a polygon file or the WKT string of the polygon geometry.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray or None</code> <p>np.ndarray or None: Processed point cloud data as a NumPy array, or None if no data is retrieved.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the input file, DTM file, or polygon file does not exist.</p> <code>ValueError</code> <p>If the input file extension is unsupported; thinning radius is non-positive; both 'hag' and 'hag_dtm' are True at the same time; or a required parameter (e.g., DTM for hag_dtm) is missing or invalid.</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def read_lidar(input_file, srs, bounds=None, thin_radius=None,\n               hag=False, hag_dtm=False, dtm=None, crop_poly=False,\n               poly=None) -&gt; np.ndarray or None:\n    \"\"\"\n    Read and process a LiDAR point cloud file using PDAL with various options.\n\n    Args:\n        input_file (str): Path to the input LiDAR file. Supported formats: .las, .laz, .copc, .copc.laz, or ept.json.\n        srs (str): Spatial Reference System (SRS) of the point cloud.\n        bounds (tuple or list, optional): Bounds for cropping data (only applies to EPT format).\n            Format: ([xmin, xmax], [ymin, ymax], [zmin, zmax]).\n        thin_radius (float, optional): Radius for thinning the point cloud. Must be positive.\n        hag (bool, optional): If True, calculate Height Above Ground (HAG) using Delaunay triangulation.\n            Defaults to False.\n        hag_dtm (bool, optional): If True, calculate Height Above Ground (HAG) using a DTM file.\n            Defaults to False.\n        dtm (str, optional): Path to the DTM (.tif) file, required if hag_dtm is True.\n        crop_poly (bool, optional): If True, crop the point cloud using a polygon. Defaults to False.\n        poly (str, optional): Path to a polygon file or the WKT string of the polygon geometry.\n\n    Returns:\n        np.ndarray or None: Processed point cloud data as a NumPy array, or None if no data is retrieved.\n\n    Raises:\n        FileNotFoundError: If the input file, DTM file, or polygon file does not exist.\n        ValueError: If the input file extension is unsupported; thinning radius is non-positive;\n            both 'hag' and 'hag_dtm' are True at the same time; or a required parameter (e.g., DTM for hag_dtm)\n            is missing or invalid.\n    \"\"\"\n    if not _is_url(input_file) and not os.path.isfile(input_file):\n        raise FileNotFoundError(f\"No such file: '{input_file}'\")\n\n    las_extensions = ('.las', '.laz')\n    copc_extensions = ('.copc', '.copc.laz')\n    ept_file = ('ept.json')\n\n    file_lower = input_file.lower()\n    if file_lower.endswith(las_extensions):\n        reader = 'readers.las'\n    elif file_lower.endswith(copc_extensions):\n        reader = 'readers.copc'\n    elif file_lower.endswith(ept_file):\n        reader = 'readers.ept'\n    else:\n        raise ValueError(\n            \"The input file must be a .las, .laz, .copc, .copc.laz file, or an ept.json file.\"\n        )\n\n    if hag and hag_dtm:\n        raise ValueError(\"Cannot use both 'hag' and 'hag_dtm' options at the same time.\")\n\n    pipeline_stages = []\n    crs_list = []\n\n    if crop_poly:\n        if not poly:\n            raise ValueError(f\"Must provide a polygon or polygon wkt if cropping to a polygon.\")\n        if poly.strip().startswith(('POLYGON', 'MULTIPOLYGON')):\n            polygon_wkt = poly\n        else:\n            if not poly or not os.path.isfile(poly):\n                raise FileNotFoundError(f\"No such polygon file: '{poly}'\")\n            polygon_wkt, crs_vector = load_polygon_from_file(poly)\n        pipeline_stages.append(_crop_polygon(polygon_wkt))\n\n    if thin_radius is not None:\n        if thin_radius &lt;= 0:\n            raise ValueError(\"Thinning radius must be a positive number.\")\n        pipeline_stages.append(_filter_radius(thin_radius))\n\n    if hag:\n        pipeline_stages.append(_hag_delaunay())\n\n    if hag_dtm:\n        if not dtm:\n            raise ValueError(\"DTM file path must be provided when 'hag_dtm' is True.\")\n        if not os.path.isfile(dtm):\n            raise FileNotFoundError(f\"No such DTM file: '{dtm}'\")\n        if not dtm.lower().endswith('.tif'):\n            raise ValueError(\"The DTM file must be a .tif file.\")\n        crs_raster = get_raster_epsg(dtm)\n        crs_list.append(crs_raster)\n        pipeline_stages.append(_hag_raster(dtm))\n\n    base_pipeline = {\n        \"type\": reader,\n        \"spatialreference\": srs,\n        \"filename\": input_file\n    }\n    if bounds and reader == 'readers.ept':\n        base_pipeline[\"bounds\"] = f\"{bounds}\"\n    main_pipeline_json = {\n        \"pipeline\": [\n            base_pipeline\n        ] + pipeline_stages\n    }\n\n    main_pipeline = pdal.Pipeline(json.dumps(main_pipeline_json))\n    main_pipeline.execute()\n\n    point_cloud = main_pipeline.arrays\n    return point_cloud if point_cloud else None\n</code></pre>"},{"location":"api/handlers/#pyforestscan.handlers.simplify_crs","title":"<code>simplify_crs(crs_list)</code>","text":"<p>Convert a list of coordinate reference system (CRS) representations to their corresponding EPSG codes.</p> <p>Parameters:</p> Name Type Description Default <code>crs_list</code> <code>list</code> <p>List of CRS definitions to be simplified. Each element may be any format accepted by pyproj.CRS (e.g., WKT string, PROJ string, EPSG code, etc.).</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List</code> <p>List of EPSG codes corresponding to the input CRS definitions.</p> <p>Raises:</p> Type Description <code>CRSError</code> <p>If any of the CRS definitions cannot be converted to an EPSG code.</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def simplify_crs(crs_list) -&gt; List:\n    \"\"\"\n    Convert a list of coordinate reference system (CRS) representations to their corresponding EPSG codes.\n\n    Args:\n        crs_list (list): List of CRS definitions to be simplified. Each element may be any format accepted by pyproj.CRS (e.g., WKT string, PROJ string, EPSG code, etc.).\n\n    Returns:\n        list: List of EPSG codes corresponding to the input CRS definitions.\n\n    Raises:\n        CRSError: If any of the CRS definitions cannot be converted to an EPSG code.\n    \"\"\"\n    epsg_codes = []\n    for crs in crs_list:\n        try:\n            crs_obj = CRS(crs)\n            epsg = crs_obj.to_epsg()\n            if epsg is None:\n                raise CRSError(f\"Cannot convert CRS '{crs}' to an EPSG code.\")\n            epsg_codes.append(epsg)\n        except CRSError as e:\n            raise CRSError(f\"Error converting CRS '{crs}': {e}\") from None\n    return epsg_codes\n</code></pre>"},{"location":"api/handlers/#pyforestscan.handlers.validate_crs","title":"<code>validate_crs(crs_list)</code>","text":"<p>Validate that all CRS (Coordinate Reference System) representations in the list are identical.</p> <p>Parameters:</p> Name Type Description Default <code>crs_list</code> <code>list</code> <p>List of coordinate reference system definitions to validate.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all CRSs match.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the CRSs do not match.</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def validate_crs(crs_list) -&gt; bool:\n    \"\"\"\n    Validate that all CRS (Coordinate Reference System) representations in the list are identical.\n\n    Args:\n        crs_list (list): List of coordinate reference system definitions to validate.\n\n    Returns:\n        bool: True if all CRSs match.\n\n    Raises:\n        ValueError: If the CRSs do not match.\n    \"\"\"\n    simplified_crs_list = simplify_crs(crs_list)\n    if not all(crs == simplified_crs_list[0] for crs in simplified_crs_list[1:]):\n        raise ValueError(\"The CRS of the inputs do not match.\")\n    return True\n</code></pre>"},{"location":"api/handlers/#pyforestscan.handlers.validate_extensions","title":"<code>validate_extensions(las_file_path, dtm_file_path)</code>","text":"<p>Validate that input file paths have the correct extensions for point cloud and DTM files.</p> <p>Parameters:</p> Name Type Description Default <code>las_file_path</code> <code>str</code> <p>File path of the point cloud file. Supported extensions are .las and .laz.</p> required <code>dtm_file_path</code> <code>str</code> <p>File path of the DTM (Digital Terrain Model) file. Supported extension is .tif.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the point cloud file does not have a .las or .laz extension.</p> <code>ValueError</code> <p>If the DTM file does not have a .tif extension.</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def validate_extensions(las_file_path, dtm_file_path):\n    \"\"\"\n    Validate that input file paths have the correct extensions for point cloud and DTM files.\n\n    Args:\n        las_file_path (str): File path of the point cloud file. Supported extensions are .las and .laz.\n        dtm_file_path (str): File path of the DTM (Digital Terrain Model) file. Supported extension is .tif.\n\n    Raises:\n        ValueError: If the point cloud file does not have a .las or .laz extension.\n        ValueError: If the DTM file does not have a .tif extension.\n    \"\"\"\n    if not las_file_path.lower().endswith(('.las', '.laz')):\n        raise ValueError(\"The point cloud file must be a .las or .laz file.\")\n    if not dtm_file_path.lower().endswith('.tif'):\n        raise ValueError(\"The DTM file must be a .tif file.\")\n</code></pre>"},{"location":"api/handlers/#pyforestscan.handlers.write_las","title":"<code>write_las(arrays, output_file, srs=None, compress=True)</code>","text":"<p>Write point cloud data to a LAS or LAZ file.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list or ndarray</code> <p>Point cloud data arrays to write.</p> required <code>output_file</code> <code>str</code> <p>Path for the output file. Must end with .las (if uncompressed) or .laz (if compressed).</p> required <code>srs</code> <code>str</code> <p>Spatial Reference System to reproject the data. If provided, reprojection is applied.</p> <code>None</code> <code>compress</code> <code>bool</code> <p>If True, write a compressed LAZ file (.laz). If False, write an uncompressed LAS file (.las). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'compress' is True and the output file extension is not .laz.</p> <code>ValueError</code> <p>If 'compress' is False and the output file extension is not .las.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pyforestscan/handlers.py</code> <pre><code>def write_las(arrays, output_file, srs=None, compress=True) -&gt; None:\n    \"\"\"\n    Write point cloud data to a LAS or LAZ file.\n\n    Args:\n        arrays (list or np.ndarray): Point cloud data arrays to write.\n        output_file (str): Path for the output file. Must end with .las (if uncompressed) or .laz (if compressed).\n        srs (str, optional): Spatial Reference System to reproject the data. If provided, reprojection is applied.\n        compress (bool, optional): If True, write a compressed LAZ file (.laz). If False, write an uncompressed LAS file (.las). Defaults to True.\n\n    Raises:\n        ValueError: If 'compress' is True and the output file extension is not .laz.\n        ValueError: If 'compress' is False and the output file extension is not .las.\n\n    Returns:\n        None\n    \"\"\"\n    output_extension = os.path.splitext(output_file)[1].lower()\n\n    if compress:\n        if output_extension != '.laz':\n            raise ValueError(\"If 'compress' is True, output file must have a .laz extension.\")\n        output_format = \"writers.las\"\n    else:\n        if output_extension != '.las':\n            raise ValueError(\"If 'compress' is False, output file must have a .las extension.\")\n        output_format = \"writers.las\"\n\n    pipeline_steps = []\n\n    if srs:\n        pipeline_steps.append({\n            \"type\": \"filters.reprojection\",\n            \"in_srs\": srs,\n            \"out_srs\": srs\n        })\n\n    pipeline_steps.append({\n        \"type\": output_format,\n        \"filename\": output_file,\n        \"minor_version\": \"4\",\n        \"extra_dims\": \"all\"\n    })\n\n    pipeline_def = {\n        \"pipeline\": pipeline_steps\n    }\n\n    pipeline_json = json.dumps(pipeline_def)\n\n    if not isinstance(arrays, list):\n        arrays = [arrays]\n\n    pipeline = pdal.Pipeline(pipeline_json, arrays=arrays)\n    pipeline.execute()\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline Module","text":""},{"location":"api/process/","title":"Process Module","text":""},{"location":"api/process/#pyforestscan.process.process_with_tiles","title":"<code>process_with_tiles(ept_file, tile_size, output_path, metric, voxel_size, voxel_height=1, buffer_size=0.1, srs=None, hag=False, hag_dtm=False, dtm=None, bounds=None, interpolation=None, remove_outliers=False, cover_min_height=2.0, cover_k=0.5)</code>","text":"<p>Process a large EPT point cloud by tiling, compute CHM or other metrics for each tile, and write the results to the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>ept_file</code> <code>str</code> <p>Path to the EPT file containing the point cloud data.</p> required <code>tile_size</code> <code>tuple</code> <p>Size of each tile as (tile_width, tile_height).</p> required <code>output_path</code> <code>str</code> <p>Directory where the output files will be saved.</p> required <code>metric</code> <code>str</code> <p>Metric to compute for each tile (\"chm\", \"fhd\", \"pai\", or \"cover\").</p> required <code>voxel_size</code> <code>tuple</code> <p>Voxel resolution as (x_resolution, y_resolution, z_resolution).</p> required <code>voxel_height</code> <code>float</code> <p>Height of each voxel in meters. Required if metric is \"pai\".</p> <code>1</code> <code>buffer_size</code> <code>float</code> <p>Fractional buffer size relative to tile size (e.g., 0.1 for 10% buffer). Defaults to 0.1.</p> <code>0.1</code> <code>srs</code> <code>str</code> <p>Spatial Reference System for the output. If None, uses SRS from the EPT file.</p> <code>None</code> <code>hag</code> <code>bool</code> <p>If True, compute Height Above Ground using Delaunay triangulation. Defaults to False.</p> <code>False</code> <code>hag_dtm</code> <code>bool</code> <p>If True, compute Height Above Ground using a provided DTM raster. Defaults to False.</p> <code>False</code> <code>dtm</code> <code>str</code> <p>Path to the DTM raster file. Required if hag_dtm is True.</p> <code>None</code> <code>bounds</code> <code>tuple</code> <p>Spatial bounds to crop the data. Must be of the form ([xmin, xmax], [ymin, ymax], [zmin, zmax]) or ([xmin, xmax], [ymin, ymax]). If None, tiling is done over the entire dataset.</p> <code>None</code> <code>interpolation</code> <code>str or None</code> <p>Interpolation method for CHM calculation (\"linear\", \"cubic\", \"nearest\", or None).</p> <code>None</code> <code>remove_outliers</code> <code>bool</code> <p>Whether to remove statistical outliers before calculating metrics. Defaults to False.</p> <code>False</code> <code>cover_min_height</code> <code>float</code> <p>Height threshold (in meters) for canopy cover (used when metric == \"cover\"). Defaults to 2.0.</p> <code>2.0</code> <code>cover_k</code> <code>float</code> <p>Beer\u2013Lambert extinction coefficient for canopy cover. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported metric is requested, if buffer or voxel sizes are invalid, or required arguments are missing.</p> <code>FileNotFoundError</code> <p>If the EPT or DTM file does not exist, or a required file for processing is missing.</p> Source code in <code>pyforestscan/process.py</code> <pre><code>def process_with_tiles(ept_file, tile_size, output_path, metric, voxel_size,\n                       voxel_height=1, buffer_size=0.1, srs=None, hag=False,\n                       hag_dtm=False, dtm=None, bounds=None, interpolation=None, remove_outliers=False,\n                       cover_min_height: float = 2.0, cover_k: float = 0.5) -&gt; None:\n    \"\"\"\n    Process a large EPT point cloud by tiling, compute CHM or other metrics for each tile,\n    and write the results to the specified output directory.\n\n    Args:\n        ept_file (str): Path to the EPT file containing the point cloud data.\n        tile_size (tuple): Size of each tile as (tile_width, tile_height).\n        output_path (str): Directory where the output files will be saved.\n        metric (str): Metric to compute for each tile (\"chm\", \"fhd\", \"pai\", or \"cover\").\n        voxel_size (tuple): Voxel resolution as (x_resolution, y_resolution, z_resolution).\n        voxel_height (float, optional): Height of each voxel in meters. Required if metric is \"pai\".\n        buffer_size (float, optional): Fractional buffer size relative to tile size (e.g., 0.1 for 10% buffer). Defaults to 0.1.\n        srs (str, optional): Spatial Reference System for the output. If None, uses SRS from the EPT file.\n        hag (bool, optional): If True, compute Height Above Ground using Delaunay triangulation. Defaults to False.\n        hag_dtm (bool, optional): If True, compute Height Above Ground using a provided DTM raster. Defaults to False.\n        dtm (str, optional): Path to the DTM raster file. Required if hag_dtm is True.\n        bounds (tuple, optional): Spatial bounds to crop the data. Must be of the form\n            ([xmin, xmax], [ymin, ymax], [zmin, zmax]) or ([xmin, xmax], [ymin, ymax]).\n            If None, tiling is done over the entire dataset.\n        interpolation (str or None, optional): Interpolation method for CHM calculation (\"linear\", \"cubic\", \"nearest\", or None).\n        remove_outliers (bool, optional): Whether to remove statistical outliers before calculating metrics. Defaults to False.\n        cover_min_height (float, optional): Height threshold (in meters) for canopy cover (used when metric == \"cover\"). Defaults to 2.0.\n        cover_k (float, optional): Beer\u2013Lambert extinction coefficient for canopy cover. Defaults to 0.5.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If an unsupported metric is requested, if buffer or voxel sizes are invalid, or required arguments are missing.\n        FileNotFoundError: If the EPT or DTM file does not exist, or a required file for processing is missing.\n    \"\"\"\n    if metric not in [\"chm\", \"fhd\", \"pai\", \"cover\"]:\n        raise ValueError(f\"Unsupported metric: {metric}\")\n\n    (min_z, max_z) = (None, None)\n    if bounds:\n        if len(bounds) == 2:\n            (min_x, max_x), (min_y, max_y) = bounds\n        else:\n            (min_x, max_x), (min_y, max_y), (min_z, max_z) = bounds\n    else:\n        min_x, max_x, min_y, max_y, min_z, max_z = get_bounds_from_ept(ept_file)\n\n    if not srs:\n        srs = get_srs_from_ept(ept_file)\n\n    num_tiles_x = int(np.ceil((max_x - min_x) / tile_size[0]))\n    num_tiles_y = int(np.ceil((max_y - min_y) / tile_size[1]))\n    total_tiles = num_tiles_x * num_tiles_y\n\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n\n    with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n        for i in range(num_tiles_x):\n            for j in range(num_tiles_y):\n                if metric == \"chm\":\n                    current_buffer_size = buffer_size\n                else:\n                    current_buffer_size = 0.0\n\n                buffer_x = current_buffer_size * tile_size[0]\n                buffer_y = current_buffer_size * tile_size[1]\n                tile_min_x = min_x + i * tile_size[0] - buffer_x\n                tile_max_x = min_x + (i + 1) * tile_size[0] + buffer_x\n                tile_min_y = min_y + j * tile_size[1] - buffer_y\n                tile_max_y = min_y + (j + 1) * tile_size[1] + buffer_y\n\n                tile_min_x = max(min_x, tile_min_x)\n                tile_max_x = min(max_x, tile_max_x)\n                tile_min_y = max(min_y, tile_min_y)\n                tile_max_y = min(max_y, tile_max_y)\n\n                if tile_max_x &lt;= tile_min_x or tile_max_y &lt;= tile_min_y:\n                    print(f\"Warning: Skipping tile ({i}, {j}) due to invalid spatial extent.\")\n                    pbar.update(1)\n                    continue\n\n                if min_z and max_z:\n                    tile_bounds = ([tile_min_x, tile_max_x], [tile_min_y, tile_max_y], [min_z, max_z])\n                else:\n                    tile_bounds = ([tile_min_x, tile_max_x], [tile_min_y, tile_max_y])\n                tile_pipeline_stages = []\n\n                if hag:\n                    tile_pipeline_stages.append(_hag_delaunay())\n                elif hag_dtm:\n                    if not dtm or not os.path.isfile(dtm):\n                        raise FileNotFoundError(f\"DTM file is required for HAG calculation using DTM: {dtm}\")\n                    cropped_dtm_path = _crop_dtm(\n                        dtm,\n                        tile_min_x, tile_min_y,\n                        tile_max_x, tile_max_y\n                    )\n                    tile_pipeline_stages.append(_hag_raster(cropped_dtm_path))\n                base_pipeline = {\n                    \"type\": \"readers.ept\",\n                    \"filename\": ept_file,\n                    \"bounds\": f\"{tile_bounds}\",\n                }\n                tile_pipeline_json = {\n                    \"pipeline\": [base_pipeline] + tile_pipeline_stages\n                }\n\n                tile_pipeline = pdal.Pipeline(json.dumps(tile_pipeline_json))\n                tile_pipeline.execute()\n                if remove_outliers:\n                    tile_points = remove_outliers_and_clean(tile_pipeline.arrays)[0]\n                else:\n                    tile_points = tile_pipeline.arrays[0]\n\n                if tile_points.size == 0:\n                    print(f\"Warning: No data in tile ({i}, {j}). Skipping.\")\n                    pbar.update(1)\n                    continue\n\n                buffer_pixels_x = int(np.ceil(buffer_x / voxel_size[0]))\n                buffer_pixels_y = int(np.ceil(buffer_y / voxel_size[1]))\n\n                if metric == \"chm\":\n                    chm, extent = calculate_chm(tile_points, voxel_size, interpolation=interpolation)\n\n                    if buffer_pixels_x * 2 &gt;= chm.shape[1] or buffer_pixels_y * 2 &gt;= chm.shape[0]:\n                        print(\n                            f\"Warning: Buffer size exceeds CHM dimensions for tile ({i}, {j}). Adjusting buffer size.\")\n                        buffer_pixels_x = max(0, chm.shape[1] // 2 - 1)\n                        buffer_pixels_y = max(0, chm.shape[0] // 2 - 1)\n\n                    chm = chm[buffer_pixels_y:-buffer_pixels_y, buffer_pixels_x:-buffer_pixels_x]\n\n                    core_extent = (\n                        tile_min_x + buffer_x,\n                        tile_max_x - buffer_x,\n                        tile_min_y + buffer_y,\n                        tile_max_y - buffer_y,\n                    )\n\n                    result_file = os.path.join(output_path, f\"tile_{i}_{j}_chm.tif\")\n                    create_geotiff(chm, result_file, srs, core_extent)\n                elif metric in [\"fhd\", \"pai\", \"cover\"]:\n                    voxels, spatial_extent = assign_voxels(tile_points, voxel_size)\n\n                    if metric == \"fhd\":\n                        result = calculate_fhd(voxels)\n                    elif metric == \"pai\":\n                        if not voxel_height:\n                            raise ValueError(f\"voxel_height is required for metric {metric}\")\n\n                        pad = calculate_pad(voxels, voxel_size[-1])\n\n                        if np.all(pad == 0):\n                            result = np.zeros((pad.shape[0], pad.shape[1]))\n                        else:\n                            result = calculate_pai(pad, voxel_height)\n                        result = np.where(np.isfinite(result), result, 0)\n                    elif metric == \"cover\":\n                        if not voxel_height:\n                            raise ValueError(f\"voxel_height is required for metric {metric}\")\n\n                        pad = calculate_pad(voxels, voxel_size[-1])\n                        if np.all(pad == 0):\n                            result = np.zeros((pad.shape[0], pad.shape[1]))\n                        else:\n                            result = calculate_canopy_cover(\n                                pad,\n                                voxel_height=voxel_height,\n                                min_height=cover_min_height,\n                                max_height=None,\n                                k=cover_k,\n                            )\n                        result = np.where(np.isfinite(result), result, 0)\n\n                    if current_buffer_size &gt; 0:\n                        if buffer_pixels_x * 2 &gt;= result.shape[1] or buffer_pixels_y * 2 &gt;= result.shape[0]:\n                            print(\n                                f\"Warning: Buffer size exceeds {metric.upper()} dimensions for tile ({i}, {j}). \"\n                                f\"Adjusting buffer size.\"\n                            )\n                            buffer_pixels_x = max(0, result.shape[1] // 2 - 1)\n                            buffer_pixels_y = max(0, result.shape[0] // 2 - 1)\n\n                        result = result[buffer_pixels_y:-buffer_pixels_y, buffer_pixels_x:-buffer_pixels_x]\n\n                    core_extent = (\n                        tile_min_x + buffer_x,\n                        tile_max_x - buffer_x,\n                        tile_min_y + buffer_y,\n                        tile_max_y - buffer_y,\n                    )\n\n                    if core_extent[1] &lt;= core_extent[0] or core_extent[3] &lt;= core_extent[2]:\n                        print(f\"Warning: Invalid core extent for tile ({i}, {j}): {core_extent}. Skipping.\")\n                        pbar.update(1)\n                        continue\n\n                    result_file = os.path.join(output_path, f\"tile_{i}_{j}_{metric}.tif\")\n                    create_geotiff(result, result_file, srs, core_extent)\n                else:\n                    raise ValueError(f\"Unsupported metric: {metric}\")\n\n                pbar.update(1)\n</code></pre>"},{"location":"api/visualize/","title":"Visualize Module","text":""},{"location":"api/visualize/#pyforestscan.visualize.plot_2d","title":"<code>plot_2d(points, x_dim='X', y_dim='Z', color_by='HeightAboveGround', color_map='viridis', colorbar_label=None, alpha=1.0, point_size=1, fig_size=None, fig_title=None, slice_dim=None, slice_val=0.0, slice_tolerance=5, save_fname=None)</code>","text":"<p>Plot a 2D scatter plot of point cloud data with customizable axes, coloring, slicing, and figure settings.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>Structured numpy array with named columns (e.g., 'X', 'Y', 'Z', 'Classification', 'HeightAboveGround', etc.).</p> required <code>x_dim</code> <code>str</code> <p>Column name for the horizontal axis. Defaults to 'X'.</p> <code>'X'</code> <code>y_dim</code> <code>str</code> <p>Column name for the vertical axis. Defaults to 'Z'.</p> <code>'Z'</code> <code>color_by</code> <code>str</code> <p>Column name to use for point color values. Defaults to 'HeightAboveGround'.</p> <code>'HeightAboveGround'</code> <code>color_map</code> <code>str</code> <p>Name of the matplotlib colormap to use. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>colorbar_label</code> <code>str</code> <p>Label for the colorbar. If None, uses <code>color_by</code>. Defaults to None.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Transparency of scatter points (0\u20131). Defaults to 1.0.</p> <code>1.0</code> <code>point_size</code> <code>int</code> <p>Size of the points in the scatter plot. Defaults to 1.</p> <code>1</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height) in inches. If None, auto-derive from aspect ratio. Defaults to None.</p> <code>None</code> <code>fig_title</code> <code>str</code> <p>Title for the plot. If None, auto-generated. Defaults to None.</p> <code>None</code> <code>slice_dim</code> <code>str</code> <p>Name of the dimension to \"slice\" (fix) at a specific value. Defaults to None.</p> <code>None</code> <code>slice_val</code> <code>float</code> <p>Value to use for slicing the <code>slice_dim</code> dimension. Defaults to 0.0.</p> <code>0.0</code> <code>slice_tolerance</code> <code>float</code> <p>Allowed difference for slice matching. Defaults to 5.</p> <code>5</code> <code>save_fname</code> <code>str</code> <p>If provided, will be forwarded to <code>plt.savefig</code> to save the figure.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>x_dim</code> or <code>y_dim</code> is not in ['X', 'Y', 'Z', 'HeightAboveGround'].</p> <code>ValueError</code> <p>If required dimension names are not present in <code>points.dtype.names</code>.</p> Source code in <code>pyforestscan/visualize.py</code> <pre><code>def plot_2d(points, x_dim='X',\n            y_dim='Z', color_by='HeightAboveGround',\n            color_map='viridis', colorbar_label=None,\n            alpha=1.0, point_size=1,\n            fig_size=None, fig_title=None,\n            slice_dim=None, slice_val=0.0,\n            slice_tolerance=5, save_fname=None\n) -&gt; None:\n    \"\"\"\n    Plot a 2D scatter plot of point cloud data with customizable axes, coloring, slicing, and figure settings.\n\n    Args:\n        points (np.ndarray): Structured numpy array with named columns (e.g., 'X', 'Y', 'Z', 'Classification', 'HeightAboveGround', etc.).\n        x_dim (str, optional): Column name for the horizontal axis. Defaults to 'X'.\n        y_dim (str, optional): Column name for the vertical axis. Defaults to 'Z'.\n        color_by (str, optional): Column name to use for point color values. Defaults to 'HeightAboveGround'.\n        color_map (str, optional): Name of the matplotlib colormap to use. Defaults to 'viridis'.\n        colorbar_label (str, optional): Label for the colorbar. If None, uses `color_by`. Defaults to None.\n        alpha (float, optional): Transparency of scatter points (0\u20131). Defaults to 1.0.\n        point_size (int, optional): Size of the points in the scatter plot. Defaults to 1.\n        fig_size (tuple, optional): Figure size as (width, height) in inches. If None, auto-derive from aspect ratio. Defaults to None.\n        fig_title (str, optional): Title for the plot. If None, auto-generated. Defaults to None.\n        slice_dim (str, optional): Name of the dimension to \"slice\" (fix) at a specific value. Defaults to None.\n        slice_val (float, optional): Value to use for slicing the `slice_dim` dimension. Defaults to 0.0.\n        slice_tolerance (float, optional): Allowed difference for slice matching. Defaults to 5.\n        save_fname (str, optional): If provided, will be forwarded to `plt.savefig` to save the figure.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If `x_dim` or `y_dim` is not in ['X', 'Y', 'Z', 'HeightAboveGround'].\n        ValueError: If required dimension names are not present in `points.dtype.names`.\n    \"\"\"\n    valid_dims = ['X', 'Y', 'Z', 'HeightAboveGround']\n    if x_dim not in valid_dims or y_dim not in valid_dims:\n        raise ValueError(f\"Invalid dimensions. Choose from: {valid_dims}\")\n\n    required_dims = [x_dim, y_dim, color_by]\n    if slice_dim is not None:\n        required_dims.append(slice_dim)\n\n    for dim in required_dims:\n        if dim not in points.dtype.names:\n            raise ValueError(f\"'{dim}' not found in array dtype names: {points.dtype.names}\")\n\n    if slice_dim and slice_dim in points.dtype.names:\n        mask = np.isclose(points[slice_dim], slice_val, atol=slice_tolerance, rtol=0)\n        points = points[mask]\n\n    if colorbar_label is None:\n        colorbar_label = color_by\n\n    x = points[x_dim]\n    y = points[y_dim]\n    colors = points[color_by]\n\n    if fig_size is None:\n        aspect_ratio = (np.max(x) - np.min(x)) / (np.max(y) - np.min(y))\n        fig_size = (10 * aspect_ratio, 10)\n\n        max_fig_size = 20\n        if max(fig_size) &gt; max_fig_size:\n            scale_factor = max_fig_size / max(fig_size)\n            fig_size = (fig_size[0] * scale_factor, fig_size[1] * scale_factor)\n\n    if fig_title is None:\n        fig_title = f'{x_dim} vs {y_dim} Colored by {color_by}'\n\n    plt.figure(figsize=fig_size)\n\n    plt.scatter(x, y, c=colors, cmap=color_map, alpha=alpha, s=point_size)\n    plt.xlabel(x_dim)\n    plt.ylabel(y_dim)\n    plt.title(fig_title)\n    plt.colorbar(label=colorbar_label)\n    if save_fname is not None:\n        plt.savefig(save_fname, dpi=300, bbox_inches='tight')\n    plt.show()\n</code></pre>"},{"location":"api/visualize/#pyforestscan.visualize.plot_metric","title":"<code>plot_metric(title, metric, extent, metric_name=None, cmap='viridis', fig_size=None, save_fname=None)</code>","text":"<p>Plot a 2D metric array as an image with geospatial extent, colorbar, and customizable settings.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the plot.</p> required <code>metric</code> <code>ndarray</code> <p>2D array representing the metric values to plot.</p> required <code>extent</code> <code>list</code> <p>List of four elements [xmin, xmax, ymin, ymax] defining the spatial extent of the plot.</p> required <code>metric_name</code> <code>str</code> <p>Label for the colorbar. If None, uses <code>title</code>. Useful for specifying units or a more detailed description.</p> <code>None</code> <code>cmap</code> <code>str</code> <p>Matplotlib colormap name for the plot. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>fig_size</code> <code>tuple</code> <p>Figure size as (width, height) in inches. If None, computed from aspect ratio and extent.</p> <code>None</code> <code>save_fname</code> <code>str</code> <p>If provided, will be forwarded to <code>plt.savefig</code> to save the figure.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pyforestscan/visualize.py</code> <pre><code>def plot_metric(title, metric, extent, metric_name=None, cmap='viridis', fig_size=None,\n                save_fname=None) -&gt; None:\n    \"\"\"\n    Plot a 2D metric array as an image with geospatial extent, colorbar, and customizable settings.\n\n    Args:\n        title (str): Title of the plot.\n        metric (np.ndarray): 2D array representing the metric values to plot.\n        extent (list): List of four elements [xmin, xmax, ymin, ymax] defining the spatial extent of the plot.\n        metric_name (str, optional): Label for the colorbar. If None, uses `title`. Useful for specifying units or a more detailed description.\n        cmap (str, optional): Matplotlib colormap name for the plot. Defaults to 'viridis'.\n        fig_size (tuple, optional): Figure size as (width, height) in inches. If None, computed from aspect ratio and extent.\n        save_fname (str, optional): If provided, will be forwarded to `plt.savefig` to save the figure.\n\n    Returns:\n        None\n    \"\"\"\n    if metric_name is None:\n        metric_name = title\n    if fig_size is None:\n        x_range = extent[1] - extent[0]\n        y_range = extent[3] - extent[2]\n        aspect_ratio = x_range / y_range\n        fig_size = (10 * aspect_ratio, 10)\n\n        max_fig_size = 20\n        if max(fig_size) &gt; max_fig_size:\n            scale_factor = max_fig_size / max(fig_size)\n            fig_size = (fig_size[0] * scale_factor, fig_size[1] * scale_factor)\n\n    plt.figure(figsize=fig_size)\n\n    plt.imshow(metric.T, extent=extent, cmap=cmap)\n    plt.colorbar(label=metric_name)\n    plt.title(title)\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    if save_fname is not None:\n        plt.savefig(save_fname, dpi=300, bbox_inches='tight')\n    plt.show()\n</code></pre>"},{"location":"api/visualize/#pyforestscan.visualize.plot_pad","title":"<code>plot_pad(pad, slice_index=None, axis='x', cmap='viridis', hag_values=None, horizontal_values=None, title=None, save_fname=None)</code>","text":"<p>Visualize 3D Plant Area Density (PAD) data as a 2D image, using projection or slicing.</p> <p>Projects or slices a 3D PAD array along a specified axis and displays the result as an image with a colormap.</p> <p>Parameters:</p> Name Type Description Default <code>pad</code> <code>ndarray</code> <p>3D numpy array of PAD values, shape (X, Y, dZ) or (dZ, Y, X).</p> required <code>slice_index</code> <code>int</code> <p>Index at which to take a 2D slice along the specified axis. If None, the PAD data is collapsed along the axis using the maximum value. Defaults to None.</p> <code>None</code> <code>axis</code> <code>str</code> <p>Axis along which to slice or project ('x' or 'y'). Defaults to 'x'.</p> <code>'x'</code> <code>cmap</code> <code>str</code> <p>Name of the matplotlib colormap for visualization. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>hag_values</code> <code>ndarray</code> <p>1D array of height-above-ground (dZ) values. If None, uses a range based on the PAD array shape.</p> <code>None</code> <code>horizontal_values</code> <code>ndarray</code> <p>1D array of horizontal axis values (X or Y, depending on <code>axis</code>). If None, uses a range based on PAD array shape.</p> <code>None</code> <code>title</code> <code>str</code> <p>Title for the plot. If None, generates an appropriate title.</p> <code>None</code> <code>save_fname</code> <code>str</code> <p>A string that will be forwarded to <code>plt.savefig</code> to save the figure.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>axis</code> is not 'x' or 'y'.</p> <code>ValueError</code> <p>If <code>slice_index</code> is out of range for the specified axis.</p> <code>ValueError</code> <p>If the length of <code>horizontal_values</code> does not match the dimension of the specified axis.</p> Source code in <code>pyforestscan/visualize.py</code> <pre><code>def plot_pad(pad, slice_index=None, axis='x', cmap='viridis',\n             hag_values=None, horizontal_values=None, title=None,\n             save_fname=None) -&gt; None:\n    \"\"\"\n    Visualize 3D Plant Area Density (PAD) data as a 2D image, using projection or slicing.\n\n    Projects or slices a 3D PAD array along a specified axis and displays the result as an image with a colormap.\n\n    Args:\n        pad (np.ndarray): 3D numpy array of PAD values, shape (X, Y, dZ) or (dZ, Y, X).\n        slice_index (int, optional): Index at which to take a 2D slice along the specified axis. If None, the PAD data is collapsed along the axis using the maximum value. Defaults to None.\n        axis (str, optional): Axis along which to slice or project ('x' or 'y'). Defaults to 'x'.\n        cmap (str, optional): Name of the matplotlib colormap for visualization. Defaults to 'viridis'.\n        hag_values (np.ndarray, optional): 1D array of height-above-ground (dZ) values. If None, uses a range based on the PAD array shape.\n        horizontal_values (np.ndarray, optional): 1D array of horizontal axis values (X or Y, depending on `axis`). If None, uses a range based on PAD array shape.\n        title (str, optional): Title for the plot. If None, generates an appropriate title.\n        save_fname (str, optional): A string that will be forwarded to `plt.savefig` to save\n            the figure.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If `axis` is not 'x' or 'y'.\n        ValueError: If `slice_index` is out of range for the specified axis.\n        ValueError: If the length of `horizontal_values` does not match the dimension of the specified axis.\n    \"\"\"\n    # Validate axis\n    if axis not in ['x', 'y']:\n        raise ValueError(f\"Invalid axis: '{axis}'. Choose from 'x' or 'y'.\")\n\n    hag_values = hag_values if hag_values is not None else np.arange(pad.shape[2])\n\n    if axis == 'x':\n        if slice_index is None:\n            pad_2d = pad.max(axis=0)\n            horizontal_axis_label = 'Y'\n            horizontal_count = pad.shape[1]\n            if horizontal_values is not None and len(horizontal_values) != horizontal_count:\n                raise ValueError(\"Length of horizontal_values does not match the Y dimension of pad.\")\n            horizontal_axis_values = horizontal_values if horizontal_values is not None else np.arange(horizontal_count)\n\n        else:\n            if slice_index &lt; 0 or slice_index &gt;= pad.shape[0]:\n                raise ValueError(f\"slice_index {slice_index} out of range for axis 'x' with size {pad.shape[0]}\")\n            pad_2d = pad[slice_index, :, :]\n            horizontal_axis_label = 'Y'\n            horizontal_axis_values = horizontal_values if horizontal_values is not None else np.arange(pad.shape[1])\n\n    else:  # axis == 'y'\n        if slice_index is None:\n            pad_2d = np.nanmax(pad, axis=1)\n            horizontal_axis_label = 'X'\n            horizontal_count = pad.shape[0]\n            if horizontal_values is not None and len(horizontal_values) != horizontal_count:\n                raise ValueError(\"Length of horizontal_values does not match the X dimension of pad.\")\n            horizontal_axis_values = horizontal_values if horizontal_values is not None else np.arange(horizontal_count)\n\n        else:\n            if slice_index &lt; 0 or slice_index &gt;= pad.shape[1]:\n                raise ValueError(f\"slice_index {slice_index} out of range for axis 'y' with size {pad.shape[1]}\")\n            pad_2d = pad[:, slice_index, :]\n            horizontal_axis_label = 'X'\n            horizontal_axis_values = horizontal_values if horizontal_values is not None else np.arange(pad.shape[0])\n\n    pad_2d = pad_2d.T\n\n    plt.figure(figsize=(10, 6))\n    img = plt.imshow(\n        pad_2d,\n        cmap=cmap,\n        origin='lower',\n        extent=(\n            horizontal_axis_values.min(),\n            horizontal_axis_values.max(),\n            hag_values.min(),\n            hag_values.max()\n        ),\n        aspect='auto'\n    )\n    plt.colorbar(img, label='PAD')\n    if title is None:\n        if slice_index is None:\n            title = f'Plant Area Density (PAD) - Collapsed by max along axis {axis}'\n        else:\n            title = f'Plant Area Density (PAD) - {horizontal_axis_label} vs dZ at slice {slice_index}'\n    plt.title(title)\n    plt.xlabel(horizontal_axis_label)\n    plt.ylabel('dZ')\n    plt.tight_layout()\n    if save_fname is not None:\n        plt.savefig(save_fname, dpi=300, bbox_inches='tight')\n    plt.show()\n</code></pre>"},{"location":"examples/calculate-forest-metrics/","title":"Calculate forest metrics","text":"In\u00a0[2]: Copied! <pre>from pyforestscan.handlers import read_lidar\nfrom pyforestscan.visualize import plot_pad, plot_metric\nfrom pyforestscan.filters import filter_hag\nfrom pyforestscan.calculate import assign_voxels, calculate_pad, calculate_pai, calculate_fhd, calculate_chm, calculate_canopy_cover\n</pre> from pyforestscan.handlers import read_lidar from pyforestscan.visualize import plot_pad, plot_metric from pyforestscan.filters import filter_hag from pyforestscan.calculate import assign_voxels, calculate_pad, calculate_pai, calculate_fhd, calculate_chm, calculate_canopy_cover In\u00a0[3]: Copied! <pre>file_path = \"../example_data/20191210_5QKB020880.laz\"\narrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\n</pre> file_path = \"../example_data/20191210_5QKB020880.laz\" arrays = read_lidar(file_path, \"EPSG:32605\", hag=True) <p>Next, we will use <code>filter_hag()</code> to remove any points below ground.</p> In\u00a0[6]: Copied! <pre>arrays = filter_hag(arrays)\npoints = arrays[0]\n</pre> arrays = filter_hag(arrays) points = arrays[0] In\u00a0[7]: Copied! <pre>voxel_resolution = (5, 5, 1) \nvoxels, extent = assign_voxels(points, voxel_resolution)\n</pre> voxel_resolution = (5, 5, 1)  voxels, extent = assign_voxels(points, voxel_resolution) In\u00a0[8]: Copied! <pre>chm, extent = calculate_chm(points, voxel_resolution)\n</pre> chm, extent = calculate_chm(points, voxel_resolution) In\u00a0[9]: Copied! <pre>plot_metric('Canopy Height Model', chm, extent, metric_name='Height (m)', cmap='viridis', fig_size=None)\n</pre> plot_metric('Canopy Height Model', chm, extent, metric_name='Height (m)', cmap='viridis', fig_size=None) In\u00a0[12]: Copied! <pre>pad = calculate_pad(voxels, voxel_resolution[-1])\n</pre> pad = calculate_pad(voxels, voxel_resolution[-1]) In\u00a0[13]: Copied! <pre>plot_pad(pad,None, axis='y', cmap='viridis')\n</pre> plot_pad(pad,None, axis='y', cmap='viridis') In\u00a0[14]: Copied! <pre>pai = calculate_pai(pad, voxel_height=1)\n</pre> pai = calculate_pai(pad, voxel_height=1) In\u00a0[15]: Copied! <pre>plot_metric('Plant Area Index', pai, extent, metric_name='PAI', cmap='viridis', fig_size=None)\n</pre> plot_metric('Plant Area Index', pai, extent, metric_name='PAI', cmap='viridis', fig_size=None) In\u00a0[16]: Copied! <pre>cover = calculate_canopy_cover(pad, voxel_height=voxel_resolution[-1], min_height=2.0, k=0.5)\n</pre> cover = calculate_canopy_cover(pad, voxel_height=voxel_resolution[-1], min_height=2.0, k=0.5) In\u00a0[17]: Copied! <pre>plot_metric('Canopy Cover (z=2 m)', cover, extent, metric_name='Cover', cmap='viridis', fig_size=None)\n</pre> plot_metric('Canopy Cover (z=2 m)', cover, extent, metric_name='Cover', cmap='viridis', fig_size=None) In\u00a0[18]: Copied! <pre>fhd = calculate_fhd(voxels)\n</pre> fhd = calculate_fhd(voxels) In\u00a0[19]: Copied! <pre>plot_metric('Foliage Height Diversity', fhd, extent, metric_name='FHD', cmap='viridis', fig_size=None)\n</pre> plot_metric('Foliage Height Diversity', fhd, extent, metric_name='FHD', cmap='viridis', fig_size=None) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/calculate-forest-metrics/#calculating-forest-metrics","title":"Calculating Forest Metrics\u00b6","text":"<p>This notebook will explain how to calculate and visualize forest metrics. Code snippets for how to get started using PyForestScan can be found in the notebook getting-started-importing-preprocessing-dtm-chm, and in the documentation.</p> <p>The example dataset is a one-square-kilometer tile derived from a 2019 aerial LiDAR survey of the Big Island of Hawaii. The data is captured over a dry forest and has a nominal pulse spacing of 0.35 meters. The data has been preprocessed to classify ground and vegetation points.</p> <p>First we will load the <code>pyforestscan</code> functions that will be used.</p>"},{"location":"examples/calculate-forest-metrics/#import-data","title":"Import Data\u00b6","text":"<p>We will begin by importing and preprocessing the data. It is important for a <code>HeightAboveGround</code> dimension to be defined, which can be accomplished by setting <code>hag=True</code> when importing data.</p>"},{"location":"examples/calculate-forest-metrics/#create-voxels","title":"Create Voxels\u00b6","text":"<p>We need to create voxels, and assign points to them. Voxel resolution must be given as a tuple with the format <code>(x_res, y_res, z_res)</code> where <code>x_res</code>, <code>y_res</code>, and <code>z_res</code> are the resolutions along the x, y, and z axes. Points within voxel bounds are assigned to them.</p>"},{"location":"examples/calculate-forest-metrics/#calculate-forest-metrics","title":"Calculate Forest Metrics\u00b6","text":""},{"location":"examples/calculate-forest-metrics/#canopy-height-model","title":"Canopy Height Model\u00b6","text":"<p>We will calculate and plot a canopy height model (CHM), showing the highest point above ground in each voxel column. <code>calculate_chm</code> will return the canopy height model along with its extent. Forest metrics are calculated in the same units as the data. In this case, since the data is in EPSG:32605, output will be in meters.</p>"},{"location":"examples/calculate-forest-metrics/#plant-area-density","title":"Plant Area Density\u00b6","text":"<p>Next we will calculate and plot plant area density (PAD), which shows density of plant matter within each voxel.</p>"},{"location":"examples/calculate-forest-metrics/#plant-area-index","title":"Plant Area Index\u00b6","text":"<p>You can calculate and plot plant area index (PAI), which quantifies the total plant surface area of above-ground plant matter (leaves, branches, and stems) per unit of ground area, which corresponds to the vertical summation of PAD.</p>"},{"location":"examples/calculate-forest-metrics/#canopy-cover","title":"Canopy Cover\u00b6","text":"<p>We can compute canopy cover at a height threshold z using the Beer\u2013Lambert relation. Here, we use z = 2 m and k = 0.5 (spherical leaf-angle assumption).</p>"},{"location":"examples/calculate-forest-metrics/#foliage-height-diversity","title":"Foliage Height Diversity\u00b6","text":"<p>We can calculate foliage height diversity (FHD). FHD is a measure of how plant material is distributed across the vertical layers of a canopy, as calculated using Shannon entropy.</p>"},{"location":"examples/getting-started-importing-preprocessing-dtm-chm/","title":"Getting started importing preprocessing dtm chm","text":"In\u00a0[1]: Copied! <pre>from pyforestscan.handlers import read_lidar, create_geotiff\nfrom pyforestscan.visualize import plot_metric\nfrom pyforestscan.filters import remove_outliers_and_clean, classify_ground_points, filter_select_ground\nfrom pyforestscan.calculate import generate_dtm, calculate_chm\n</pre> from pyforestscan.handlers import read_lidar, create_geotiff from pyforestscan.visualize import plot_metric from pyforestscan.filters import remove_outliers_and_clean, classify_ground_points, filter_select_ground from pyforestscan.calculate import generate_dtm, calculate_chm In\u00a0[2]: Copied! <pre>file_path = \"../example_data/20191210_5QKB020880.laz\"\narrays = read_lidar(file_path, \"EPSG:32605\")\n</pre> file_path = \"../example_data/20191210_5QKB020880.laz\" arrays = read_lidar(file_path, \"EPSG:32605\") <p>Now that the data has been loaded, we can clean the data by removing outliers and classifying ground points.</p> In\u00a0[3]: Copied! <pre>cleaned_arrays = remove_outliers_and_clean(arrays, mean_k=8, multiplier=3.0)\nclassified_arrays = classify_ground_points(cleaned_arrays)\n</pre> cleaned_arrays = remove_outliers_and_clean(arrays, mean_k=8, multiplier=3.0) classified_arrays = classify_ground_points(cleaned_arrays) In\u00a0[4]: Copied! <pre>ground_points = filter_select_ground(classified_arrays)\ndtm, extent = generate_dtm(ground_points, resolution=10.0)\n</pre> ground_points = filter_select_ground(classified_arrays) dtm, extent = generate_dtm(ground_points, resolution=10.0) <p>The DTM can be exported as a geotiff which can then be opened in a GIS for further analysis.</p> In\u00a0[5]: Copied! <pre>create_geotiff(dtm, \"../example_data/20191210_5QKB020880_DS05_dtm.tif\", \"EPSG:32605\", extent)\n</pre> create_geotiff(dtm, \"../example_data/20191210_5QKB020880_DS05_dtm.tif\", \"EPSG:32605\", extent) <p>We can also visualize this DTM using the <code>plot_metric</code> function.</p> In\u00a0[6]: Copied! <pre>plot_metric('Digital Terrain Model', dtm, extent, metric_name='Elevation (m)',cmap='viridis', fig_size=None)\n</pre> plot_metric('Digital Terrain Model', dtm, extent, metric_name='Elevation (m)',cmap='viridis', fig_size=None) In\u00a0[7]: Copied! <pre>arrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\n</pre> arrays = read_lidar(file_path, \"EPSG:32605\", hag=True) In\u00a0[8]: Copied! <pre>chm, extent = calculate_chm(arrays[0], (1,1,1))\n</pre> chm, extent = calculate_chm(arrays[0], (1,1,1)) <p>Ploting and export can be accomplished the same way as DTMs.</p> In\u00a0[9]: Copied! <pre>create_geotiff(chm, \"/Users/iosefa/repos/obia/docs/example_data/chm037.tif\", \"EPSG:32605\", extent)\n</pre> create_geotiff(chm, \"/Users/iosefa/repos/obia/docs/example_data/chm037.tif\", \"EPSG:32605\", extent) In\u00a0[10]: Copied! <pre>plot_metric(\"Canopy Height Model\", chm, extent, metric_name='Height (m)', cmap='viridis', fig_size=None)\n</pre> plot_metric(\"Canopy Height Model\", chm, extent, metric_name='Height (m)', cmap='viridis', fig_size=None) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/getting-started-importing-preprocessing-dtm-chm/#getting-started-importing-preprocessing-dtms-and-chms","title":"Getting Started: Importing, Preprocessing, DTMs and CHMs\u00b6","text":"<p>This notebook will explain how to load and run some of the basic functions to import and process data, create a digital terrain model, a canopy height model, and visualize and export results. Code snippets for more functions can be found in the calculate-forest-metrics notebook, and in the documentation.</p> <p>The example dataset is a one-square-kilometer tile derived from a 2019 aerial lidar survey of the Big Island of Hawaii. The data is captured over a dry forest and has a nominal pulse spacing of 0.35 meters. The data has been preprocessed to classify ground and vegetation points.</p> <p>First we will load the <code>pyforestscan</code> functions that will be used.</p>"},{"location":"examples/getting-started-importing-preprocessing-dtm-chm/#importing-and-preprocessing-the-data","title":"Importing and Preprocessing the Data\u00b6","text":"<p>We will create a digital terrain model (DTM). We will begin by importing and cleaning the data. For this example, we will act as if the data has not been preprocessed to classify ground.</p> <p>We will read the lidar data:</p>"},{"location":"examples/getting-started-importing-preprocessing-dtm-chm/#creating-a-dtm","title":"Creating a DTM\u00b6","text":"<p>To create a DTM, we will begin by extracting only the ground points. Then we can use those ground points to generate the DTM.</p>"},{"location":"examples/getting-started-importing-preprocessing-dtm-chm/#creating-a-chm","title":"Creating a CHM\u00b6","text":"<p>Next we will create a canopy height model (CHM) from a point cloud array. This array must have a HeightAboveGround dimension defined. This can be added easily when importing the data by setting <code>hag=True</code>. In addition to your point cloud array, you must also define the resolution. The resolution must be given as a tuple with the format <code>(x_res, y_res, z_res)</code> where <code>x_res</code>, <code>y_res</code>, and <code>z_res</code> are the resolutions along the x, y, and z axes. <code>calculate_chm</code> will return the canopy height model along with its extent.</p>"},{"location":"examples/working-with-large-point-clouds/","title":"Working with large point clouds","text":"In\u00a0[1]: Copied! <pre>import geopandas as gpd\nimport numpy as np\n\nfrom pyforestscan.handlers import read_lidar, create_geotiff, write_las\nfrom pyforestscan.visualize import plot_metric, plot_2d\nfrom pyforestscan.calculate import assign_voxels, calculate_pad, calculate_pai, calculate_fhd, calculate_chm\nfrom pyforestscan.process import process_with_tiles\nfrom pyforestscan.utils import get_srs_from_ept\n</pre> import geopandas as gpd import numpy as np  from pyforestscan.handlers import read_lidar, create_geotiff, write_las from pyforestscan.visualize import plot_metric, plot_2d from pyforestscan.calculate import assign_voxels, calculate_pad, calculate_pai, calculate_fhd, calculate_chm from pyforestscan.process import process_with_tiles from pyforestscan.utils import get_srs_from_ept In\u00a0[2]: Copied! <pre>bounds = ([-17348441.871880997,-17347398.335829224],[2245235.283966082,2246320.888103429])\nept = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/HI_Hawaii_Island_2017/ept.json\"\nept_srs = get_srs_from_ept(ept)\npointclouds = read_lidar(ept, ept_srs, bounds, hag=True)\n</pre> bounds = ([-17348441.871880997,-17347398.335829224],[2245235.283966082,2246320.888103429]) ept = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/HI_Hawaii_Island_2017/ept.json\" ept_srs = get_srs_from_ept(ept) pointclouds = read_lidar(ept, ept_srs, bounds, hag=True) <p>Once the file is read, we can build the voxels and calculate forest metrics:</p> In\u00a0[3]: Copied! <pre>voxel_resolution = (5, 5, 1) \nvoxels, extent = assign_voxels(pointclouds[0], voxel_resolution)\n</pre> voxel_resolution = (5, 5, 1)  voxels, extent = assign_voxels(pointclouds[0], voxel_resolution) In\u00a0[4]: Copied! <pre>pad = calculate_pad(voxels, voxel_resolution[-1])\npai = calculate_pai(pad, 1)\n</pre> pad = calculate_pad(voxels, voxel_resolution[-1]) pai = calculate_pai(pad, 1) In\u00a0[5]: Copied! <pre>plot_metric(\"Plant Area Index\", pai, extent, metric_name='Plant Area Index', cmap='viridis', fig_size=None)\n</pre> plot_metric(\"Plant Area Index\", pai, extent, metric_name='Plant Area Index', cmap='viridis', fig_size=None) In\u00a0[\u00a0]: Copied! <pre>bounds = ([-17348441.871880997,-17347398.335829224],[2245235.283966082,2246320.888103429])\nept = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/HI_Hawaii_Island_2017/ept.json\"\n\nprocess_with_tiles(\n    ept, \n    (500, 500), \n    \"../example_data/tiles\", \n    \"pai\", \n    (5, 5, 1), \n    buffer_size=0.1,\n    bounds=bounds,\n    hag_dtm=True,\n    dtm=\"../example_data/USGS_1M_4_x83y219_HI_Hawaii_Island_Lidar_NOAA_2017_B17_cropped_3857.tif\",\n    remove_outliers=True\n)\n</pre> bounds = ([-17348441.871880997,-17347398.335829224],[2245235.283966082,2246320.888103429]) ept = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/HI_Hawaii_Island_2017/ept.json\"  process_with_tiles(     ept,      (500, 500),      \"../example_data/tiles\",      \"pai\",      (5, 5, 1),      buffer_size=0.1,     bounds=bounds,     hag_dtm=True,     dtm=\"../example_data/USGS_1M_4_x83y219_HI_Hawaii_Island_Lidar_NOAA_2017_B17_cropped_3857.tif\",     remove_outliers=True ) <pre>Processing tiles:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 7/9 [01:32&lt;00:21, 10.93s/it]</pre> <p>And we can plot this geotiff using rasterio or load it into a GIS.</p> In\u00a0[\u00a0]: Copied! <pre>import rasterio\nimport matplotlib.pyplot as plt\n\ntif_path = \"../example_data/tiles/tile_0_0_pai.tif\"\n\nwith rasterio.open(tif_path) as src:\n    raster = src.read(1)\n    metadata = src.meta\n\nplt.figure(figsize=(10, 8))\nplt.imshow(raster, cmap='viridis')\nplt.colorbar(label=\"Values\")\nplt.title(\"Raster Plot\")\nplt.xlabel(\"Column Index\")\nplt.ylabel(\"Row Index\")\nplt.show()\n</pre> import rasterio import matplotlib.pyplot as plt  tif_path = \"../example_data/tiles/tile_0_0_pai.tif\"  with rasterio.open(tif_path) as src:     raster = src.read(1)     metadata = src.meta  plt.figure(figsize=(10, 8)) plt.imshow(raster, cmap='viridis') plt.colorbar(label=\"Values\") plt.title(\"Raster Plot\") plt.xlabel(\"Column Index\") plt.ylabel(\"Row Index\") plt.show() In\u00a0[\u00a0]: Copied! <pre>crowns = gpd.read_file(\"../example_data/test_segment.gpkg\")\ncrown_geom = crowns.geometry.iloc[0]\nminx, miny, maxx, maxy = crown_geom.bounds\nbounds = ([minx, maxx], [miny, maxy])\nept_srs = \"EPSG:3857\"\npointclouds = read_lidar(ept, ept_srs, bounds, crop_poly=True, poly=crown_geom.wkt, hag=True)\n</pre> crowns = gpd.read_file(\"../example_data/test_segment.gpkg\") crown_geom = crowns.geometry.iloc[0] minx, miny, maxx, maxy = crown_geom.bounds bounds = ([minx, maxx], [miny, maxy]) ept_srs = \"EPSG:3857\" pointclouds = read_lidar(ept, ept_srs, bounds, crop_poly=True, poly=crown_geom.wkt, hag=True) <p>Now to plot with points colored by height above ground:</p> In\u00a0[\u00a0]: Copied! <pre>plot_2d(pointclouds[0], x_dim='X', y_dim='Z', alpha=0.5, point_size=50, fig_size=(10, 10))\n</pre> plot_2d(pointclouds[0], x_dim='X', y_dim='Z', alpha=0.5, point_size=50, fig_size=(10, 10)) <p>And we can calculate metrics for the tree by setting the voxel size to be that of the bounds of the clipped point:</p> In\u00a0[\u00a0]: Copied! <pre>voxel_resolution = (maxx - minx, maxy - miny, 1) \nvoxels, extent = assign_voxels(pointclouds[0], voxel_resolution)\ncanopy_height, extent = calculate_chm(pointclouds[0], voxel_resolution)\npad = calculate_pad(voxels, voxel_resolution[-1], beer_lambert_constant=0.4)\npai = calculate_pai(pad, 1)\nfhd = calculate_fhd(voxels)\n# assign_voxels will round the voxel shape to the nearest integer, so padding with 0s is introduced here \n# (since the point cloud was clipped to the crown polygon earlier). We can simply take the max to get values\nprint(\"canopy height:\", np.nanmax(canopy_height))\nprint(\"plant area index:\", np.nanmax(pai))\nprint(\"foliage height diversity:\", np.nanmax(fhd))\n</pre> voxel_resolution = (maxx - minx, maxy - miny, 1)  voxels, extent = assign_voxels(pointclouds[0], voxel_resolution) canopy_height, extent = calculate_chm(pointclouds[0], voxel_resolution) pad = calculate_pad(voxels, voxel_resolution[-1], beer_lambert_constant=0.4) pai = calculate_pai(pad, 1) fhd = calculate_fhd(voxels) # assign_voxels will round the voxel shape to the nearest integer, so padding with 0s is introduced here  # (since the point cloud was clipped to the crown polygon earlier). We can simply take the max to get values print(\"canopy height:\", np.nanmax(canopy_height)) print(\"plant area index:\", np.nanmax(pai)) print(\"foliage height diversity:\", np.nanmax(fhd))"},{"location":"examples/working-with-large-point-clouds/#working-with-large-point-clouds","title":"Working with Large Point Clouds\u00b6","text":"<p>This notebook will explain how use PyForestScan with large data sets. Code snippets for how to get started using PyForestScan or calculate key forest metrics can be found in the notebooks getting-started-importing-preprocessing-dtm-chm or calculate-forest-metrics, and in the documentation.</p> <p>For this tutorial, we will use lidar that is hosted by USGS in their 3D Elevation Program (3DEP) (USGS 2025). This data contains over 113 billion points and has a nominal pulse spacing of 0.35 meters covering the Big Island of Hawaii.</p> <p>First we will load the necessary python packages and functions.</p>"},{"location":"examples/working-with-large-point-clouds/#import-data","title":"Import Data\u00b6","text":"<p>PyForestScan leverages PDAL and Entwine to read EPT and COPC data formats. EPT and COPC are optimized for handling very large point clouds and have spatial knowlege of the data, allowing us to access metadata for the entire dataset while working with extracts. We will use this to extract only points that fit within the one kilometer tile that we explored in the other examples.</p>"},{"location":"examples/working-with-large-point-clouds/#creating-tiled-metrics","title":"Creating Tiled Metrics\u00b6","text":"<p>We can also generate tiled metrics from the EPT dataset. In this example, we will read from the same EPT file to produce 1000\u202fm\u202f\u00d7\u202f1000\u202fm tiles of Plant Area Index (PAI) across a larger region, writing each tile as a GeoTIFF. To compute Height Above Ground (HAG) for these tiles, we rely on an existing digital terrain model (DTM).</p> <p>Note on HAG with Small Tiles: If you attempt to compute HAG from scratch (e.g., using a Delaunay triangulation) on very small tiles, you may end up with inconsistent ground estimates at tile edges or an insufficient number of ground returns. For best results, either:</p> <ol> <li><p>Use larger tiles so there is enough data in each tile to estimate ground reliably, or</p> </li> <li><p>Compute HAG once over the entire region (or sufficiently large areas), write the pointcloud with the HeightAboveGround dimension, then tile the data afterward, or</p> </li> <li><p>Use an existing DTM (as in this example) to derive HAG consistently across tiles.</p> </li> </ol>"},{"location":"examples/working-with-large-point-clouds/#reading-and-clipping-to-polygons","title":"Reading and Clipping to Polygons\u00b6","text":"<p>We can also read from a large point cloud and only load the points clipped to polygons by making use of the read EPT capabilities and the ability to clip to polygons. This will also allow us to read points that belong to a single and calculate these metrics at the tree level. For example, we can calculate the PAI and FHD for a given tree.</p>"},{"location":"examples/working-with-large-point-clouds/#reference-and-acknowledgements","title":"Reference and Acknowledgements\u00b6","text":"<p>Map services and data available from U.S. Geological Survey, National Geospatial Program.</p> <p>USGS (2025). USGS 3DEP LiDAR Point Clouds was accessed on 4/3/2025 from https://registry.opendata.aws/usgs-lidar)</p>"},{"location":"usage/digital-terrain-models/","title":"Digital Terrain Models","text":"<p>Given a classified point cloud, pyforestscan can create, save, and visualize digital terrain models (DTMs) as geotiffs:</p> <pre><code>from pyforestscan.calculate import generate_dtm\nfrom pyforestscan.filters import filter_select_ground\nfrom pyforestscan.handlers import create_geotiff\nfrom pyforestscan.visualize import plot_metric\n\nground_points = filter_select_ground(classified_arrays)\n\ndtm, extent = generate_dtm(ground_points, resolution=10.0)\n\ncreate_geotiff(dtm, \"../example_data/20191210_5QKB020880_DS05_dtm.tif\", \"EPSG:32605\", extent)\n\nplot_metric('Digital Terrain Model', dtm, extent, metric_name='Elevation (m)', cmap='viridis', fig_size=None)\n</code></pre> <p></p>"},{"location":"usage/getting-started-import-and-preprocess/","title":"Importing, Preprocessing, and Writing Data","text":"<p>To use pyforestscan, first import it in your Python project:</p> <pre><code>import pyforestscan\n</code></pre> <p>Then, you can use it to load point cloud data and extract forest structure metrics. </p> <p>The following sections will provide an overview of usage of the major functions of pyforestscan. For a complete reference of all functions in pyforestscan, please check the API documentation. For comprehensive examples of these functions, please see the example jupyter notebooks. </p>"},{"location":"usage/getting-started-import-and-preprocess/#importing-point-cloud-data","title":"Importing Point Cloud Data","text":"<p>pyforestscan supports reading from the following point cloud data formats:</p> <ul> <li>las</li> <li>laz</li> <li>copc</li> <li>ept</li> </ul> <p>and reading point clouds is done using the <code>read_lidar</code> function:</p> <pre><code>from pyforestscan.handlers import read_lidar\n\nfile_path = \"../example_data/20191210_5QKB020880.laz\"\narrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\npointcloud = arrays[0]\n</code></pre>"},{"location":"usage/getting-started-import-and-preprocess/#preprocessing-point-cloud-data","title":"Preprocessing Point Cloud Data","text":"<p>pyforestscan provides some basic functionality to help preprocess point cloud data. Many of these functions are wrapped PDAL routines. For example, to remove outliers and classify ground points:</p> <pre><code>from pyforestscan.filters import remove_outliers_and_clean, classify_ground_points\n\ncleaned_arrays = remove_outliers_and_clean(arrays, mean_k=8, multiplier=3.0)\nclassified_arrays = classify_ground_points(cleaned_arrays)\n</code></pre>"},{"location":"usage/getting-started-import-and-preprocess/#exporting-point-clouds","title":"Exporting Point Clouds","text":"<p>pyforestscan supports exporting processed point clouds to las and laz formats. To export a point cloud as a LAZ file:</p> <pre><code>from pyforestscan.handlers import write_las\n\nwrite_las(classified_arrays, \"/path/to/exported_file.las\", srs=\"EPSG:32605\", compress=True)\n</code></pre>"},{"location":"usage/forest-structure/canopy_cover/","title":"Canopy Cover (GEDI-style)","text":""},{"location":"usage/forest-structure/canopy_cover/#definition","title":"Definition","text":"<p>Canopy cover at height z is the fraction of ground area occluded by vegetation above z meters (Height Above Ground). Following GEDI, we model the gap probability using the Beer\u2013Lambert law:</p> <p>Cover(z) = 1 \u2212 exp(\u2212k \u00b7 PAI_above(z))</p> <p>Where: - <code>k</code> is the extinction coefficient (default 0.5 for spherical leaf-angle assumption). - <code>PAI_above(z)</code> is the vertical integral of Plant Area Density (PAD) above height <code>z</code>.</p> <p>Common choices: - Tree canopy cover (GEDI-like): z = 2.0 m. - Total vegetative cover: z = 0.0 m.</p>"},{"location":"usage/forest-structure/canopy_cover/#usage","title":"Usage","text":"<pre><code>from pyforestscan.handlers import read_lidar\nfrom pyforestscan.filters import filter_hag\nfrom pyforestscan.calculate import assign_voxels, calculate_pad, calculate_canopy_cover\nfrom pyforestscan.visualize import plot_metric\n\nfile_path = \"../example_data/20191210_5QKB020880.laz\"\n\n# Ensure HAG is present for PAD/PAI calculations\narrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\narrays = filter_hag(arrays)\npoints = arrays[0]\n\nvoxel_resolution = (5, 5, 1)  # dx, dy, dz in meters\nvoxels, extent = assign_voxels(points, voxel_resolution)\n\npad = calculate_pad(voxels, voxel_resolution[-1])\n\n# GEDI-like canopy cover at z=2 m\ncover = calculate_canopy_cover(pad, voxel_height=voxel_resolution[-1], min_height=2.0, k=0.5)\n\nplot_metric('Canopy Cover (z=2 m)', cover, extent, metric_name='Cover', cmap='viridis')\n</code></pre>"},{"location":"usage/forest-structure/canopy_cover/#notes","title":"Notes","text":"<ul> <li>As z increases, canopy cover decreases because less foliage lies above higher thresholds.</li> <li><code>calculate_canopy_cover</code> returns NaN where PAD is entirely missing in the integration range.</li> <li>When tiling large EPT datasets, use <code>process_with_tiles(..., metric=\"cover\", voxel_height=..., cover_min_height=2.0, cover_k=0.5)</code> to write GeoTIFF tiles.</li> </ul>"},{"location":"usage/forest-structure/chm/","title":"Canopy Height Models (CHM)","text":""},{"location":"usage/forest-structure/chm/#theory","title":"Theory","text":"<p>Canopy height is given as the maximum height above ground for a point within each grid cell or for all points within a given polygon.</p> \\[ H_{\\text{canopy}} = \\max(HAG_{\\text{points}}) \\] <p>Where:</p> <ul> <li>\\( H_{\\text{canopy}} \\) is the canopy height for the grid cell or polygon.</li> <li>\\( HAG_{\\text{points}} \\) represents the set of heights above ground of all points within a given grid cell or polygon.</li> </ul>"},{"location":"usage/forest-structure/chm/#calculating-chm","title":"Calculating CHM","text":"<p>To calculate canopy height and generate a canopy height model:</p> <pre><code>from pyforestscan.handlers import read_lidar\nfrom pyforestscan.visualize import plot_metric\nfrom pyforestscan.calculate import calculate_chm\n\nfile_path = \"../example_data/20191210_5QKB020880.laz\"\narrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\nchm, extent = calculate_chm(arrays[0], (1,1,1))\nplot_metric(\"Canopy Height Model\", chm, extent, metric_name='Height (m)', cmap='viridis', fig_size=None)\n</code></pre> <p></p>"},{"location":"usage/forest-structure/chm/#gridded-chm","title":"Gridded CHM","text":"<p>PyForestScan uses PDAL and Entwine to read and process large files stored in the EPT format. These can be used to create gridded metrics, like CHM. To create a 1km grid of CHM:</p> <pre><code>from pyforestscan.process import process_with_tiles\n\nept = \"../example_data/ept/ept.json\"\nept_srs = \"EPSG:32605\"\n\nprocess_with_tiles(\n    ept, \n    (1000, 1000), \n    \"../example_data/tiles\", \n    \"chm\", \n    (5, 5, 1), \n    buffer_size=0.15,\n    srs=\"EPSG:32605\"\n)\n</code></pre> <p>If the EPT covers a very large area and the gridded metrics should only cover a fraction of that area, you can provide the tile processor with the bounds of the region that you want to tile:</p> <pre><code>bounds = ([202000.000, 205000.000], [2186999.980, 2190000.000])\nprocess_with_tiles(\n    ept, \n    (1000, 1000), \n    \"../example_data/tiles\", \n    \"pai\", \n    (5, 5, 1), \n    buffer_size=0.15,\n    srs=\"EPSG:32605\",\n    bounds=bounds\n)\n</code></pre>"},{"location":"usage/forest-structure/chm/#canopy-height-for-abstract-polygons","title":"Canopy Height for Abstract Polygons","text":"<p>It is also possible to read a point cloud and clip it to a polygon in order to extract metrics for that region clipped by the polygon. For example, with the polygon of a tree, we can clip the points to that tree and extact the canopy height:</p> <p><pre><code>import geopandas as gpd\nfrom pyforestscan.handlers import read_lidar\nfrom pyforestscan.visualize import plot_2d\n\nept = \"../example_data/ept/ept.json\"\nept_srs = \"EPSG:32605\"\n\ncrowns = gpd.read_file(\"../example_data/test_segment.gpkg\")\ncrown_geom = crowns.geometry.iloc[0]\nminx, miny, maxx, maxy = crown_geom.bounds\nbounds = ([minx, maxx], [miny, maxy])\npointclouds = read_lidar(ept, ept_srs, bounds, crop_poly=True, poly=crown_geom.wkt)\nplot_2d(pointclouds[0], x_dim='X', y_dim='Z', alpha=0.5, point_size=50, fig_size=(10, 10))\n</code></pre> </p>"},{"location":"usage/forest-structure/fhd/","title":"Foliage Height Diversity (FHD)","text":""},{"location":"usage/forest-structure/fhd/#theory","title":"Theory","text":"<p>Foliage Height Diversity (FHD) is a metric that quantifies the vertical distribution of plant material in the forest canopy. It is based on Shannon entropy and calculated using methods derived from Hurlbert (1971) and MacArthur &amp; MacArthur (1961).</p> \\[ FHD = - \\sum_{i=1}^{n} p_i \\ln(p_i) \\] <p>Where: -   \\( FHD \\) is the Foliage Height Diversity. -   \\( p_i \\) is the proportion of total plant material in voxel \\( i \\) relative to the entire vertical column. -   \\( n \\) is the number of vertical layers in the canopy.</p> <p>FHD provides an indication of how plant material is distributed vertically, with higher values suggesting a more even distribution of foliage across different height levels.</p>"},{"location":"usage/forest-structure/fhd/#calculating-fhd","title":"Calculating FHD","text":"<p>To calculate FHD:</p> <pre><code>from pyforestscan.handlers import read_lidar\nfrom pyforestscan.visualize import plot_metric\nfrom pyforestscan.filters import filter_hag\nfrom pyforestscan.calculate import assign_voxels, calculate_fhd\n\nfile_path = \"../example_data/20191210_5QKB020880.laz\"\narrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\narrays = filter_hag(arrays)\npoints = arrays[0]\n\nvoxel_resolution = (5, 5, 1) \nvoxels, extent = assign_voxels(points, voxel_resolution)\n\nfhd = calculate_fhd(voxels)\nplot_metric('Foliage Height Diversity', fhd, extent, metric_name='FHD', cmap='viridis', fig_size=None)\n</code></pre> <p></p>"},{"location":"usage/forest-structure/fhd/#references","title":"References","text":"<p>Hurlbert, Stuart H. 1971. \"The Nonconcept of Species Diversity: A Critique and Alternative Parameters.\" Ecology 52 (4): 577--86. https://doi.org/10.2307/1934145.</p> <p>MacArthur, Robert H., and John W. MacArthur. 1961. \"On Bird Species Diversity.\" Ecology 42 (3): 594--98. https://doi.org/10.2307/1932254.</p>"},{"location":"usage/forest-structure/intro/","title":"Introduction","text":"<p>The core forest structural metrics generated by PyForestScan are based on well-established methods in ecology.</p> <p></p> Forest structural metrics as calculated from points within voxels. Voxel resolution is given by \u0394x, \u0394y, \u0394z. <p>Most calculations for the metrics follows the method outlined in Kamoske et al. 2019.</p>"},{"location":"usage/forest-structure/intro/#metrics","title":"Metrics","text":"<ul> <li>Canopy Height Models (CHM)</li> <li>Plant Area Density (PAD)</li> <li>Plant Area Index (PAI)</li> <li>Foliage Height Diversity (FHD)</li> <li>Canopy Cover</li> </ul>"},{"location":"usage/forest-structure/intro/#references","title":"References","text":"<p>Kamoske, Aaron G., Kyla M. Dahlin, Scott C. Stark, and Shawn P. Serbin. 2019. \"Leaf Area Density from Airborne LiDAR: Comparing Sensors and Resolutions in a Temperate Broadleaf Forest Ecosystem.\" Forest Ecology and Management 433 (February): 364--75. https://doi.org/10.1016/j.foreco.2018.11.017.</p>"},{"location":"usage/forest-structure/pad/","title":"Plant Area Density (PAD)","text":""},{"location":"usage/forest-structure/pad/#theory","title":"Theory","text":"<p>Plant Area Density (PAD) is a measure of the amount of plant material in a vertical slice of the forest, derived from airborne LiDAR data. The calculation follows the method outlined in Kamoske et al. 2019.</p> \\[PAD_{i-1,i} = \\ln\\left(\\frac{S_e}{S_t}\\right) \\frac{1}{k \\Delta z}\\] <p>Where:</p> <ul> <li><code>PAD_{i-1,i}</code> represents the Plant Area Density between two adjacent     voxels, indexed by ( i-1 ) and ( i ).</li> <li>( S_e ) is the number of lidar pulses entering the voxel.</li> <li>( S_t ) is the number of lidar pulses exiting the voxel.</li> <li>( k ) is the extinction coefficient from the Beer-Lambert Law.</li> <li>( Delta z ) is the height of each voxel.</li> </ul> <p>The equation calculates the natural logarithm of the ratio of entering and exiting lidar pulses, scaled by the inverse of the extinction coefficient and the voxel height. This quantifies the density of plant material between the two voxels.</p>"},{"location":"usage/forest-structure/pad/#calculating-pad","title":"Calculating PAD","text":"<p>To calculate PAD:</p> <pre><code>from pyforestscan.handlers import read_lidar\nfrom pyforestscan.visualize import plot_pad\nfrom pyforestscan.filters import filter_hag\nfrom pyforestscan.calculate import assign_voxels, calculate_pad\n\nfile_path = \"../example_data/20191210_5QKB020880.laz\"\narrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\narrays = filter_hag(arrays)\npoints = arrays[0]\n\nvoxel_resolution = (5, 5, 1) \nvoxels, extent = assign_voxels(points, voxel_resolution)\n\npad = calculate_pad(voxels, voxel_resolution[-1])\nplot_pad(pad, 5, axis='y', cmap='viridis')\n</code></pre> <p></p>"},{"location":"usage/forest-structure/pad/#references","title":"References","text":"<p>Kamoske, Aaron G., Kyla M. Dahlin, Scott C. Stark, and Shawn P. Serbin. 2019. \"Leaf Area Density from Airborne LiDAR: Comparing Sensors and Resolutions in a Temperate Broadleaf Forest Ecosystem.\" Forest Ecology and Management 433 (February): 364--75. https://doi.org/10.1016/j.foreco.2018.11.017.</p>"},{"location":"usage/forest-structure/pai/","title":"Plant Area Index (PAI)","text":""},{"location":"usage/forest-structure/pai/#theory","title":"Theory","text":"<p>Plant Area Index (PAI) is a measure of the total plant material in a vertical column of the forest. It is calculated as the sum of the Plant Area Density (PAD) across all layers in the canopy.</p> \\[ PAI = \\sum_{i=1}^{n} PAD_{i-1,i} \\] <p>Where: -   \\( PAI \\) is the Plant Area Index. -   \\( PAD_{i-1,i} \\) is the Plant Area Density between adjacent layers \\( i-1 \\) and \\( i \\). -   \\( n \\) is the total number of layers in the vertical column.</p> <p>PAI provides an aggregated view of plant material from the ground to the top of the canopy by summing the PAD for each vertical layer.</p>"},{"location":"usage/forest-structure/pai/#calculating-pai","title":"Calculating PAI","text":"<p>To calculate PAI, first calculate PAD, then:</p> <pre><code>from pyforestscan.handlers import read_lidar\nfrom pyforestscan.visualize import plot_metric\nfrom pyforestscan.filters import filter_hag\nfrom pyforestscan.calculate import assign_voxels, calculate_pad, calculate_pai\n\nfile_path = \"../example_data/20191210_5QKB020880.laz\"\narrays = read_lidar(file_path, \"EPSG:32605\", hag=True)\narrays = filter_hag(arrays)\npoints = arrays[0]\n\nvoxel_resolution = (5, 5, 1) \nvoxels, extent = assign_voxels(points, voxel_resolution)\n\npad = calculate_pad(voxels, voxel_resolution[-1])\npai = calculate_pai(pad)\nplot_metric('Plant Area Index', pai, extent, metric_name='PAI', cmap='viridis', fig_size=None)\n</code></pre> <p></p>"}]}